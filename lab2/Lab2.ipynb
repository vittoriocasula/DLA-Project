{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. If you haven't already, it is highly recommended to:\n",
    "\n",
    "- Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "- Watch (and potentially _code along_) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "\n",
    "In this first exercise you will train a _small_ autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89ab57",
   "metadata": {},
   "source": [
    "Questo è il mio primo esercizio di NLP e ne approfitto per fare pratica con l'ecosistema di Hugging Face.\n",
    "Il focus di questo esercizio non era quello di capire quale modello ha risultati migliori e quindi ho evitato di inserire Weigths and Bias perchè ritengo che un notebook è sufficiente per sperimentare velocemente con il testo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2491b",
   "metadata": {},
   "source": [
    "## Installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53e28a",
   "metadata": {},
   "source": [
    "You have to run the following instruction to create your conda env:\n",
    "\n",
    "- conda create --name Lab2_DLA\n",
    "- conda activate Lab2_DLA\n",
    "- conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "- conda install -c conda-forge transformers\n",
    "- pip install scikit-learn\n",
    "- pip install evaluate\n",
    "- pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c5cb7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:23:56.577948Z",
     "iopub.status.busy": "2024-01-10T11:23:56.577061Z",
     "iopub.status.idle": "2024-01-10T11:24:00.209188Z",
     "shell.execute_reply": "2024-01-10T11:24:00.208319Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GenerationConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import warnings\n",
    "\n",
    "# serve per evitare gli warnings molto comuni quando lavori con hugging face\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08084c8",
   "metadata": {},
   "source": [
    "### Device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5d76e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.213860Z",
     "iopub.status.busy": "2024-01-10T11:24:00.213613Z",
     "iopub.status.idle": "2024-01-10T11:24:00.246101Z",
     "shell.execute_reply": "2024-01-10T11:24:00.245102Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:\" + str(i))  # cuda:0 , cuda:1\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # yes, i have a MacBookPro with M1 Pro\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "print(\n",
    "    \"GPU: \" + torch.cuda.get_device_name(i)\n",
    "    if device == torch.device(\"cuda:\" + str(i))\n",
    "    else \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc783b",
   "metadata": {},
   "source": [
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf20d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.250756Z",
     "iopub.status.busy": "2024-01-10T11:24:00.250606Z",
     "iopub.status.idle": "2024-01-10T11:24:00.254336Z",
     "shell.execute_reply": "2024-01-10T11:24:00.253661Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # how many indipendent sequences will be process in parallel ?\n",
    "# what is the maximum context length for predictions ?\n",
    "block_size = 256  # context_lenght\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "max_new_tokens = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c6229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.257818Z",
     "iopub.status.busy": "2024-01-10T11:24:00.257691Z",
     "iopub.status.idle": "2024-01-10T11:24:00.263568Z",
     "shell.execute_reply": "2024-01-10T11:24:00.262914Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4eb598",
   "metadata": {},
   "source": [
    "### Pipeline seguita\n",
    "\n",
    "- scarica il file\n",
    "- elimina la parte iniziale (che contiene informazioni sulla licenza) per fare il training\n",
    "- tokenizza il testo\n",
    "- crea il modello\n",
    "- fase di training (per poche epoche)\n",
    "- guarda la loss\n",
    "- genera testo e valuta qualitativamente i risultati\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca670b4",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692883c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.267445Z",
     "iopub.status.busy": "2024-01-10T11:24:00.267314Z",
     "iopub.status.idle": "2024-01-10T11:24:00.273874Z",
     "shell.execute_reply": "2024-01-10T11:24:00.273175Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"dataset\\n\")\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(f\"lenght of dataset in characters: {len(text)}\\n\")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'chars: {\"\".join(chars)}')\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "# print(text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d67ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.277975Z",
     "iopub.status.busy": "2024-01-10T11:24:00.277853Z",
     "iopub.status.idle": "2024-01-10T11:24:00.283018Z",
     "shell.execute_reply": "2024-01-10T11:24:00.282096Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a mapping from character to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# encoder: take a string, output a list of integer\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# decoder: take a list of integer, output a string\n",
    "decode = lambda l: \"\".join(itos[i] for i in l)\n",
    "\n",
    "print(encode(\"Hi Vittorio\"))\n",
    "print(decode(encode(\"Hi Vittorio\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2cff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.287575Z",
     "iopub.status.busy": "2024-01-10T11:24:00.287443Z",
     "iopub.status.idle": "2024-01-10T11:24:00.307505Z",
     "shell.execute_reply": "2024-01-10T11:24:00.306562Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n",
    "# the 100 characters we looked at earlier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3ad5e",
   "metadata": {},
   "source": [
    "### Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61949169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.311507Z",
     "iopub.status.busy": "2024-01-10T11:24:00.311361Z",
     "iopub.status.idle": "2024-01-10T11:24:00.315578Z",
     "shell.execute_reply": "2024-01-10T11:24:00.314649Z"
    }
   },
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f26272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.319540Z",
     "iopub.status.busy": "2024-01-10T11:24:00.319386Z",
     "iopub.status.idle": "2024-01-10T11:24:00.327729Z",
     "shell.execute_reply": "2024-01-10T11:24:00.326771Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Context lenght: {block_size}\\n\")\n",
    "print(\"Chunk of data:\")\n",
    "train_data[: block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6390bf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.332017Z",
     "iopub.status.busy": "2024-01-10T11:24:00.331846Z",
     "iopub.status.idle": "2024-01-10T11:24:00.339500Z",
     "shell.execute_reply": "2024-01-10T11:24:00.338514Z"
    }
   },
   "outputs": [],
   "source": [
    "# how labeling works in NLP task\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "print(f\"Context lenght: {block_size}\")\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")\n",
    "\n",
    "    if t == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610cf919",
   "metadata": {},
   "source": [
    "### Get batch of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b12fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:00.343919Z",
     "iopub.status.busy": "2024-01-10T11:24:00.343734Z",
     "iopub.status.idle": "2024-01-10T11:24:02.348201Z",
     "shell.execute_reply": "2024-01-10T11:24:02.347197Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(low=0, high=len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(\"input: \")\n",
    "print(f\"\\n{xb.shape}\\n{xb}\\n\")\n",
    "print(\"target: \")\n",
    "print(f\"\\n{yb.shape}\\n{yb}\\n\")\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "# how batching works\n",
    "for b in range(batch_size):  # 0 --> 4\n",
    "    for t in range(block_size):  # 0 --> 8\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context} the target is {target}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba6366",
   "metadata": {},
   "source": [
    "### Trasformers Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82041f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:02.353475Z",
     "iopub.status.busy": "2024-01-10T11:24:02.353341Z",
     "iopub.status.idle": "2024-01-10T11:24:02.367605Z",
     "shell.execute_reply": "2024-01-10T11:24:02.366683Z"
    }
   },
   "outputs": [],
   "source": [
    "# thanks to Karpathy\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=n_embd\n",
    "        )\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device)\n",
    "        )  # (T, C )\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B,T, vocab_size)\n",
    "        # n.b: cross entropy vuole i logits con shape (B * T, C) e non (B, T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # function to generate from the model\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size torkens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)  # logits.shape = (B * T, C)\n",
    "            # focus only in the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append samples index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of sel-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1)  # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, T) @ (B,T,C) --> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallell\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Trasformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000d15e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:02.371812Z",
     "iopub.status.busy": "2024-01-10T11:24:02.371688Z",
     "iopub.status.idle": "2024-01-10T11:24:02.488437Z",
     "shell.execute_reply": "2024-01-10T11:24:02.487450Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a model and optimizer\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076c308",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392c48e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:02.492662Z",
     "iopub.status.busy": "2024-01-10T11:24:02.492530Z",
     "iopub.status.idle": "2024-01-10T11:24:02.498620Z",
     "shell.execute_reply": "2024-01-10T11:24:02.497696Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"./text_generation_model/\", exist_ok=True)\n",
    "\n",
    "path_models = \"./text_generation_model/model_text_generation.pth\"\n",
    "\n",
    "if not os.path.exists(path_models):  # se non esiste fai l'addestramento\n",
    "    for iter in range(max_iters):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss(model)\n",
    "            print(\n",
    "                f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "        xb, yb = get_batch(\"train\")\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.save(model.state_dict(), path_models)\n",
    "else:\n",
    "    print(f\"Model exist on path {path_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a78a5",
   "metadata": {},
   "source": [
    "### Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5807b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:02.503202Z",
     "iopub.status.busy": "2024-01-10T11:24:02.503080Z",
     "iopub.status.idle": "2024-01-10T11:24:51.594747Z",
     "shell.execute_reply": "2024-01-10T11:24:51.593918Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(path_models))\n",
    "# # questa istruzione crea problemi se addestro su gpu su papavero e provo a caricare i pesi su mps o cpu,\n",
    "# usa torhc.load specificando map_location il dispositivo mps o cpu\n",
    "# torch.load(path_models, map_location=device)\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(path_models, map_location=device)\n",
    ")  # carico il modello appena addestrato\n",
    "\n",
    "# generates text\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=max_new_tokens)[0].tolist()))\n",
    "# il testo generato a seguito dell'addestramento dovrebbe avere senso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ce8e2",
   "metadata": {},
   "source": [
    "Il testo generato ha una grande somiglianza con lo stile di scrittura del noto poeta Dante. Si può notare come le frasi siano corte e risultano suddivise in gruppi di tre frasi (chiamate \"terzine\"). Ogni terzina ha le due ultime due frasi \"indentate\" a destra. La maggiorparte delle parole hanno un senso nella lingua italiana, altre invece contengono dei semplici errori di lettere all'interno.\n",
    "Altra fattore da evidenziare è che il modello spesso riesce a capire quando siamo all'inizio di una conversazione e viene posto il carattere \":\" prima del suo inizio.\n",
    "\n",
    "Sicuramente con un addestramento con più epoche avremmo un risultato molto migliore ricco di altre caratteristiche dello stile di dante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a _huge_ variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "\n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text).\n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists).\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4cd2cc",
   "metadata": {},
   "source": [
    "### Pipeline svolta\n",
    "\n",
    "- installa HF transformers\n",
    "- è stato necessario eseguire anche --> conda install -c conda-forge huggingface_hub\n",
    "- encode text into sub-word tokens with GPT2Tokenizer\n",
    "- istanzia GPT2Tokenizer e converti il testo in token interi\n",
    "- confronta la lunghezza dell'input (testo) con la lunghezza della sequenza codificata (lunghezza della sequenza di interi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79cc86",
   "metadata": {},
   "source": [
    "### Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68f0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:51.607909Z",
     "iopub.status.busy": "2024-01-10T11:24:51.607787Z",
     "iopub.status.idle": "2024-01-10T11:24:51.914054Z",
     "shell.execute_reply": "2024-01-10T11:24:51.913300Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# stampa gli attributi e i metodi che posso invocare con l'oggetto tokenizer\n",
    "# print(*dir(tokenizer), sep=\"\\n\")\n",
    "# print(tokenizer.get_vocab) # important params settings\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\\n\")\n",
    "\n",
    "print(tokenizer.encode(\"Hi Vittorio\"))  # dict with keys: input_ids, attention_mask\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hi Vittorio\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da50c6",
   "metadata": {},
   "source": [
    "Funzione per vedere come viene codificata una data frase. Nella stampa non viene stampata la codifica se risulta essere molto ingombrante nell'output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb89674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:51.918973Z",
     "iopub.status.busy": "2024-01-10T11:24:51.918801Z",
     "iopub.status.idle": "2024-01-10T11:24:51.923034Z",
     "shell.execute_reply": "2024-01-10T11:24:51.922328Z"
    }
   },
   "outputs": [],
   "source": [
    "def view_encoded_text(text, huge=False):\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    contents = f\"{text} --> {encoded_text}\" if huge == False else \"\"\n",
    "    print(\n",
    "        f\"° length text (in characters): {len(text)} - length encoded text: {encoded_text.shape[1]}\\t {contents}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f895e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:51.927260Z",
     "iopub.status.busy": "2024-01-10T11:24:51.927089Z",
     "iopub.status.idle": "2024-01-10T11:24:51.934738Z",
     "shell.execute_reply": "2024-01-10T11:24:51.934025Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Esempi:\\n\")\n",
    "view_encoded_text(\"Good Morning!\")\n",
    "view_encoded_text(\" Good Morning!\")\n",
    "view_encoded_text(\"  Good Morning!\")\n",
    "view_encoded_text(\"   Good Morning!\")\n",
    "view_encoded_text(\"Good Morning!   \")\n",
    "view_encoded_text(\"Good Morning!  \")\n",
    "view_encoded_text(\"Good Morning! \")\n",
    "view_encoded_text(\"Good Morning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed4517",
   "metadata": {},
   "source": [
    "Da notare che la lunghezza del testo codificato tiene conto del numero di parole e della punteggiatura (carattere spazio incluso).\n",
    "Per evitare di scrivere un programma che conti il numero di parole e il numero di caratteri della punteggiatura contiamo i caratteri della testo come avevamo fatto nel precedente esercizio.\n",
    "\n",
    "Si può notare che all'aumentare/diminuire della lunghezza del testo aumenta/diminuisce la lunghezza della sequenza di interi. Però fra le due lenght non c'è una corrispondenza biunivoca (per esempio ad un testo di 13 caratteri potrebbero corrispondere una codifica con 3 o 4 interi come si può vedere negli esempi sopra).\n",
    "Si può notare inoltre che l'aggiunta di tanti spazi fa aumentare la lunghezza della sequenza di interi. Questo potrebbe essere uno svantaggio in certi casi. In genere l'aggiunta di più di uno spazio nella scrittura con tastiera qwerty è un'errore di battitura che però un modello di linguaggio codifica come input di testo grezzo e potrebbe creare delle relazioni tra parole successive.\n",
    "\n",
    "Proviamo con esempi di testo più lunghi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040b5e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:51.939202Z",
     "iopub.status.busy": "2024-01-10T11:24:51.939016Z",
     "iopub.status.idle": "2024-01-10T11:24:51.946670Z",
     "shell.execute_reply": "2024-01-10T11:24:51.945965Z"
    }
   },
   "outputs": [],
   "source": [
    "view_encoded_text(\n",
    "    \"My name is Vittorio Casula. I'm 26 years old. I graduated in Computer Engineering in 2021 at University of Florence.\",\n",
    "    huge=True,\n",
    ")\n",
    "view_encoded_text(\n",
    "    \"My best purchase of the last 2 years is my Macbook Pro with M1 Pro Processor. Thanks to it, I'm able to run fast my Machine Learning Script on my laptop.\",\n",
    "    huge=True,\n",
    ")\n",
    "view_encoded_text(\n",
    "    \"However, I believe that the optimal setup for working in the Machine Learning field should involve two devices: the first being a laptop (not necessarily a MacBook) with a large-capacity battery, and the second being a server (connected via SSH) on which you can run your scripts without utilizing the resources of the laptop.\",\n",
    "    huge=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7f5de",
   "metadata": {},
   "source": [
    "### Compare between text length and encoded text length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cbcac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:51.950976Z",
     "iopub.status.busy": "2024-01-10T11:24:51.950787Z",
     "iopub.status.idle": "2024-01-10T11:24:52.758631Z",
     "shell.execute_reply": "2024-01-10T11:24:52.758006Z"
    }
   },
   "outputs": [],
   "source": [
    "# proviamo a fare una stringa dinamica che diventa sempre più lunga\n",
    "\n",
    "dynamic_str = \"\"\n",
    "len_max = 1024\n",
    "chars = f\"ABCDEFGHILMNOPQRSTUVXZabcdefghijlmnopqrstuvxz0123456789\"\n",
    "indices = torch.randint(low=0, high=len(chars), size=(len_max,)).tolist()\n",
    "\n",
    "lengths_text = []\n",
    "lengths_encoded_text = []\n",
    "for i in range(len(indices)):\n",
    "    dynamic_str += chars[indices[i]]\n",
    "    lengths_text.append(len(dynamic_str))\n",
    "    lengths_encoded_text.append(\n",
    "        tokenizer(dynamic_str, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    )\n",
    "\n",
    "plt.title(\"Lunghezza del testo (in caratteri) vs Lunghezza del testo codificato\")\n",
    "plt.xlabel(\"Lunghezza (in caratteri) del testo\")\n",
    "plt.ylabel(\"Lunghezza del testo codificato\")\n",
    "\n",
    "plt.plot(lengths_text, lengths_encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9d489",
   "metadata": {},
   "source": [
    "Quando la lunghezza dei caratteri è pari a 1000 la lunghezza del testo codificato è pari a 800. Questo gap diventerà tanto più grande al crescere del testo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a _prompt_ in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is _greedy_ which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dbd21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:52.763298Z",
     "iopub.status.busy": "2024-01-10T11:24:52.763173Z",
     "iopub.status.idle": "2024-01-10T11:24:54.728467Z",
     "shell.execute_reply": "2024-01-10T11:24:54.727969Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = \"My name is Vittorio. I'm AI student at University of Florence.\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "# print(*inputs)  # get the keys of returned dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e16a6",
   "metadata": {},
   "source": [
    "### Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abfc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model.generate(**inputs) # warning: Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "print(f\"Generated text: \\n{tokenizer.decode(outputs[0])}\")\n",
    "# if you want decode multiple interger sequence you should use batch_decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86461d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:54.733153Z",
     "iopub.status.busy": "2024-01-10T11:24:54.732978Z",
     "iopub.status.idle": "2024-01-10T11:24:57.057757Z",
     "shell.execute_reply": "2024-01-10T11:24:57.056352Z"
    }
   },
   "outputs": [],
   "source": [
    "# interesting warning:Using the model-agnostic default `max_length` (=20) to control thegeneration length.\n",
    "# We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=100\n",
    ")\n",
    "# print(outputs)\n",
    "print(\"Generated text: \\n\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0894e8",
   "metadata": {},
   "source": [
    "Come vediamo il testo generato contiene il testo iniziamente codificato e una frase generata che si ripete fino a raggiungere il numero massimo di token specificato. Non ha molto senso questo testo generato. Proviamo a sperimentare con i parametri \"do_sample\" e \"temperature\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd431678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:57.062993Z",
     "iopub.status.busy": "2024-01-10T11:24:57.062747Z",
     "iopub.status.idle": "2024-01-10T11:24:57.067906Z",
     "shell.execute_reply": "2024-01-10T11:24:57.066916Z"
    }
   },
   "outputs": [],
   "source": [
    "# check \"do_sample\" and \"temperature\" parameters\n",
    "# link: https://huggingface.co/docs/transformers/generation_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69610d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:57.071356Z",
     "iopub.status.busy": "2024-01-10T11:24:57.071192Z",
     "iopub.status.idle": "2024-01-10T11:24:57.077201Z",
     "shell.execute_reply": "2024-01-10T11:24:57.075821Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.generation_config)\n",
    "# Printing out the model.generation_config reveals only the values that are different from the default generation configuration,\n",
    "# and does not list any of the default values.\n",
    "\n",
    "# The default generation configuration limits the size of the output combined with the input prompt to a maximum of 20 tokens\n",
    "# to avoid running into resource limitations. The default decoding strategy is greedy search, which is the simplest decoding\n",
    "# strategy that picks a token with the highest probability as the next token. For many tasks and small output sizes this works well.\n",
    "# However, when used to generate longer outputs, greedy search can start producing highly repetitive results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e8b10",
   "metadata": {},
   "source": [
    "### Customize Text Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02da57",
   "metadata": {},
   "source": [
    "do_sample: if set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling,\n",
    "Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary\n",
    "with various strategy-specific adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9f19c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:57.081691Z",
     "iopub.status.busy": "2024-01-10T11:24:57.081526Z",
     "iopub.status.idle": "2024-01-10T11:24:59.254916Z",
     "shell.execute_reply": "2024-01-10T11:24:59.254389Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=100, do_sample=True\n",
    ")\n",
    "# print(outputs)\n",
    "print(\"Generated text: \\n\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f96b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:24:59.259629Z",
     "iopub.status.busy": "2024-01-10T11:24:59.259486Z",
     "iopub.status.idle": "2024-01-10T11:25:00.340453Z",
     "shell.execute_reply": "2024-01-10T11:25:00.339923Z"
    }
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "# print(outputs)\n",
    "print(\"Generated text: \\n\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f75701",
   "metadata": {},
   "source": [
    "Assisted decoding is a modification of the decoding strategies above that uses an assistant model with the same tokenizer (ideally a much smaller model) to greedily generate a few candidate tokens. The main model then validates the candidate tokens in a single forward pass, which speeds up the decoding process. Currently, only greedy search and sampling are supported with assisted decoding, and doesn’t support batched inputs. To learn more about assisted decoding, check this blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0f985",
   "metadata": {},
   "source": [
    "When using assisted decoding with sampling methods, you can use the temperature argument to control the randomness just like in multinomial sampling. However, in assisted decoding, reducing the temperature will help improving latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280ca30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:00.345583Z",
     "iopub.status.busy": "2024-01-10T11:25:00.345387Z",
     "iopub.status.idle": "2024-01-10T11:25:05.252291Z",
     "shell.execute_reply": "2024-01-10T11:25:05.251598Z"
    }
   },
   "outputs": [],
   "source": [
    "temperature = [0.2, 0.4, 0.6, 0.8, 1]\n",
    "for temp in temperature:\n",
    "    outputs = model.generate(\n",
    "        **inputs, generation_config=generation_config, do_sample=True, temperature=temp\n",
    "    )\n",
    "    print(f\"\\n° Generated text (temperature = {temp}):\")\n",
    "    print(f\"{tokenizer.decode(outputs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d5bda",
   "metadata": {},
   "source": [
    "Testo generato molto vario al crescere del parametro temperature. Per piccoli valori di temperature si ha il fenomeno della ripetizione del testo. Per valori intermedi si ha la ripetizione del testo con qualche piccola variazione (ad esempio: \"I'm a teacher of music.\" e \"I'm teacher of literature\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, _at least_ one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "- Since GPT2 is a _autoregressive_ model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select _one_ to use).\n",
    "\n",
    "- BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "- The first _two_ exercises below can probably be done _without_ any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "# Exercise 3.1: Training a Text Classifier (easy) <span style=\"color: red;\">(DONE)</span>\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a _moderately_ sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM _exclusively_ as a feature extractor and then train a shallow model.\n",
    "\n",
    "# Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a _moderately_ sized one and train a model to answer contextualized multiple-choice questions. You _might_ be able to avoid fine-tuning by training a simple model to _rank_ the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure _similarity_ between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the _retrieval_ problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc5b0d",
   "metadata": {},
   "source": [
    "### Model: GPT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27a1fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:05.254610Z",
     "iopub.status.busy": "2024-01-10T11:25:05.254400Z",
     "iopub.status.idle": "2024-01-10T11:25:05.258076Z",
     "shell.execute_reply": "2024-01-10T11:25:05.257376Z"
    }
   },
   "outputs": [],
   "source": [
    "name_model_pretrained = \"gpt2\"\n",
    "os.makedirs(\"./text_classification_model/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9725fc",
   "metadata": {},
   "source": [
    "Dataset scelto: tweet_eval (link: https://huggingface.co/datasets/tweet_eval)\n",
    "\n",
    "TweetEval consists of seven heterogenous tasks in Twitter, all framed as multi-class tweet classification. The tasks include - irony, hate, offensive, stance, emoji, emotion, and sentiment. All tasks have been unified into the same benchmark, with each dataset presented in the same format and with fixed training, validation and test splits.\n",
    "The text in the dataset is in English, as spoken by Twitter users.\n",
    "\n",
    "# selected task: emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d600c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:05.259535Z",
     "iopub.status.busy": "2024-01-10T11:25:05.259320Z",
     "iopub.status.idle": "2024-01-10T11:25:11.118488Z",
     "shell.execute_reply": "2024-01-10T11:25:11.117712Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b497b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:11.122409Z",
     "iopub.status.busy": "2024-01-10T11:25:11.122274Z",
     "iopub.status.idle": "2024-01-10T11:25:11.127184Z",
     "shell.execute_reply": "2024-01-10T11:25:11.126532Z"
    }
   },
   "outputs": [],
   "source": [
    "# some example\n",
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b60e3f",
   "metadata": {},
   "source": [
    "### Data Fields\n",
    "\n",
    "For emoji config:\n",
    "\n",
    "- text: a string feature containing the tweet\n",
    "- label: an int classification label with the following mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8f5fe",
   "metadata": {},
   "source": [
    "![labels](./img/labels.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314fcd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:11.131585Z",
     "iopub.status.busy": "2024-01-10T11:25:11.131463Z",
     "iopub.status.idle": "2024-01-10T11:25:11.135334Z",
     "shell.execute_reply": "2024-01-10T11:25:11.134660Z"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"Red Heart\",\n",
    "    1: \"Smiling Face with Heart-Eyes\",\n",
    "    2: \"Face with Tears of Joy\",\n",
    "    3: \"Two Hearts\",\n",
    "    4: \"Fire\",\n",
    "    5: \"Smiling Face with Similing Eyes\",\n",
    "    6: \"Similing Face with Sunglasses\",\n",
    "    7: \"Sparkles\",\n",
    "    8: \"Blue Heart\",\n",
    "    9: \"Face Blowing a Kiss\",\n",
    "    10: \"Camera\",\n",
    "    11: \"Flag United States\",\n",
    "    12: \"Sun\",\n",
    "    13: \"Purple Heart\",\n",
    "    14: \"Winking Face\",\n",
    "    15: \"Hundred Points\",\n",
    "    16: \"Beaming Face with Smiling Eyes\",\n",
    "    17: \"Christmas Tree\",\n",
    "    18: \"Camera with Flash\",\n",
    "    19: \"Winking Face with Tongue\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf48cb",
   "metadata": {},
   "source": [
    "### Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cca58b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:11.138672Z",
     "iopub.status.busy": "2024-01-10T11:25:11.138553Z",
     "iopub.status.idle": "2024-01-10T11:25:11.779349Z",
     "shell.execute_reply": "2024-01-10T11:25:11.778512Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(name_model_pretrained)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f42595",
   "metadata": {},
   "source": [
    "Si può notare che a partire dal modello preaddestrato con la classe AutoTokenizer si può ricavare il Tokenizer usato per l'addestramento di tale modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10444c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:11.783535Z",
     "iopub.status.busy": "2024-01-10T11:25:11.783365Z",
     "iopub.status.idle": "2024-01-10T11:25:13.082340Z",
     "shell.execute_reply": "2024-01-10T11:25:13.081093Z"
    }
   },
   "outputs": [],
   "source": [
    "# funzione per calcolare l'accuracy (viene usata la funzione di sk-learn)\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# funzione per effettuare la tokenizzazione di tutto il dataset (di tutti gli split)\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "# tokenizzazione del dataset\n",
    "tokenized_tweet_eval = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ba2af",
   "metadata": {},
   "source": [
    "Con le istruzioni seguenti settiamo il mostro modello pre-addestrato GPT2\n",
    "Viene impostato l'ID del token di padding (pad_token_id) del modello uguale all'ID del token di fine frase (eos_token_id) del modello.\n",
    "\n",
    "Il token di padding viene utilizzato nelle operazioni di elaborazione del linguaggio naturale, in particolare durante il padding delle sequenze di testo di lunghezza variabile in modo che abbiano tutte la stessa lunghezza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239a3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:13.087136Z",
     "iopub.status.busy": "2024-01-10T11:25:13.086772Z",
     "iopub.status.idle": "2024-01-10T11:25:14.203934Z",
     "shell.execute_reply": "2024-01-10T11:25:14.203143Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    name_model_pretrained, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579be806",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7d1ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:14.208514Z",
     "iopub.status.busy": "2024-01-10T11:25:14.208309Z",
     "iopub.status.idle": "2024-01-10T11:25:15.926579Z",
     "shell.execute_reply": "2024-01-10T11:25:15.925738Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./text_classification_model/emoji/\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    # lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    optim=\"adamw_torch\"\n",
    "    # no_cuda=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_tweet_eval[\"train\"],\n",
    "    eval_dataset=tokenized_tweet_eval[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca30bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:25:15.931355Z",
     "iopub.status.busy": "2024-01-10T11:25:15.931174Z",
     "iopub.status.idle": "2024-01-10T12:18:06.167298Z",
     "shell.execute_reply": "2024-01-10T12:18:06.166282Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64e4b3",
   "metadata": {},
   "source": [
    "Questo addestramento è stato fatto anche per un numero di epoche ben più grande (30 epoche) ma il risultato in termini di validation accuracy non è cambiato molto (a partire dalla 3°/4° epoca si mantiene costante). Ho deciso quindi di effettuare tutti i training fino a 5 epoche.\n",
    "\n",
    "Come vediamo sia la training loss sia la validation loss tendono a scendere. La validation loss però da una certa epoca in poi inizierà a salire (ma non troppo) generando overfitting. Questa situazione è stata vista solo per il task di emoji classification.\n",
    "\n",
    "Come vediamo la accuracy sul validation set è piuttosto bassa (0.28) e questo si è verificato solo per il task di emoji classification. Per altri task si ha delle performance molto più alte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ec714",
   "metadata": {},
   "source": [
    "### Evalution on Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ba1d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:18:06.174812Z",
     "iopub.status.busy": "2024-01-10T12:18:06.174612Z",
     "iopub.status.idle": "2024-01-10T12:18:06.947215Z",
     "shell.execute_reply": "2024-01-10T12:18:06.946136Z"
    }
   },
   "outputs": [],
   "source": [
    "path_models = \"./text_classification_model/emoji/best_model/\"\n",
    "trainer.save_model(path_models)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path_models)\n",
    "trainer.evaluate(eval_dataset=tokenized_tweet_eval[\"test\"])  # on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1d951",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf734cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:20:54.884566Z",
     "iopub.status.busy": "2024-01-10T12:20:54.884369Z",
     "iopub.status.idle": "2024-01-10T12:20:54.890012Z",
     "shell.execute_reply": "2024-01-10T12:20:54.889035Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(sequence_to_classify):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path_models).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name_model_pretrained)\n",
    "    encoded_sequence = tokenizer(sequence_to_classify, return_tensors=\"pt\")\n",
    "    encoded_sequence = encoded_sequence.to(device)\n",
    "    logits = model(**encoded_sequence).logits\n",
    "    predicted_class = logits.argmax().item()\n",
    "    print(\n",
    "        f\"{sequence_to_classify} --> Classe predetta: {predicted_class} ({id2label[predicted_class]}) - score = {torch.softmax(logits, dim=1).squeeze(0)[predicted_class]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6338827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:20:54.894496Z",
     "iopub.status.busy": "2024-01-10T12:20:54.894323Z",
     "iopub.status.idle": "2024-01-10T12:21:03.482994Z",
     "shell.execute_reply": "2024-01-10T12:21:03.481368Z"
    }
   },
   "outputs": [],
   "source": [
    "inference(\"i love you\")\n",
    "inference(\"i missed you\")\n",
    "inference(\"Happy new Year!\")\n",
    "inference(\"Merry Christmas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff6685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:21:03.488239Z",
     "iopub.status.busy": "2024-01-10T12:21:03.488028Z",
     "iopub.status.idle": "2024-01-10T12:21:05.442882Z",
     "shell.execute_reply": "2024-01-10T12:21:05.441229Z"
    }
   },
   "outputs": [],
   "source": [
    "inference(\"Do you take a photo together?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f71d1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:21:05.448043Z",
     "iopub.status.busy": "2024-01-10T12:21:05.447826Z",
     "iopub.status.idle": "2024-01-10T12:21:09.559133Z",
     "shell.execute_reply": "2024-01-10T12:21:09.558532Z"
    }
   },
   "outputs": [],
   "source": [
    "inference(\"I'd rather burn myself\")\n",
    "inference(\"fuck you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4258f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:21:09.564600Z",
     "iopub.status.busy": "2024-01-10T12:21:09.564360Z",
     "iopub.status.idle": "2024-01-10T12:21:17.932155Z",
     "shell.execute_reply": "2024-01-10T12:21:17.931551Z"
    }
   },
   "outputs": [],
   "source": [
    "inference(\"make america great again\")\n",
    "inference(\"max points\")\n",
    "inference(\"highest quality\")\n",
    "inference(\"best quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31699422",
   "metadata": {},
   "source": [
    "# selected task: emotion\n",
    "\n",
    "Task: classificare se un certo tweet appartiene ad una delle seguenti classi:\n",
    "\n",
    "- 0: anger\n",
    "- 1: joy\n",
    "- 2: optimism\n",
    "- 3: sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9f4d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:21:17.938074Z",
     "iopub.status.busy": "2024-01-10T12:21:17.937842Z",
     "iopub.status.idle": "2024-01-10T12:21:23.417391Z",
     "shell.execute_reply": "2024-01-10T12:21:23.416788Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tweet_eval\", \"emotion\")\n",
    "\n",
    "id2label = {0: \"anger\", 1: \"joy\", 2: \"optimism\", 3: \"sadness\"}\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5266d7ed",
   "metadata": {},
   "source": [
    "Tokenizer &Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cdabc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:21:23.431935Z",
     "iopub.status.busy": "2024-01-10T12:21:23.431743Z",
     "iopub.status.idle": "2024-01-10T12:27:33.041906Z",
     "shell.execute_reply": "2024-01-10T12:27:33.040877Z"
    }
   },
   "outputs": [],
   "source": [
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name_model_pretrained)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "os.environ[\n",
    "    \"CUDA_VISIBLE_DEVICES\"\n",
    "] = \"1\"  # la gpu:0 di papavero era occupata, con questa istruzione faccio in modo di vedere solo la gpu:1\n",
    "\n",
    "tokenized_tweet_eval = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    name_model_pretrained, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./text_classification_model/emotion/\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    # lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    optim=\"adamw_torch\"\n",
    "    # no_cuda=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_tweet_eval[\"train\"],\n",
    "    eval_dataset=tokenized_tweet_eval[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d1dd57",
   "metadata": {},
   "source": [
    "Ottima performance in questo caso. Le due loss hanno la tendenza a decrescere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f11863",
   "metadata": {},
   "source": [
    "Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8b755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:27:33.047391Z",
     "iopub.status.busy": "2024-01-10T12:27:33.047203Z",
     "iopub.status.idle": "2024-01-10T12:27:44.038540Z",
     "shell.execute_reply": "2024-01-10T12:27:44.037269Z"
    }
   },
   "outputs": [],
   "source": [
    "path_models = \"./text_classification_model/emotion/best_model/\"\n",
    "trainer.save_model(path_models)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path_models)\n",
    "trainer.evaluate(eval_dataset=tokenized_tweet_eval[\"test\"])  # on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d320e4",
   "metadata": {},
   "source": [
    "Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebceba46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:27:44.043864Z",
     "iopub.status.busy": "2024-01-10T12:27:44.043495Z",
     "iopub.status.idle": "2024-01-10T12:27:52.581374Z",
     "shell.execute_reply": "2024-01-10T12:27:52.579761Z"
    }
   },
   "outputs": [],
   "source": [
    "inference(\"Don't give up!\")\n",
    "inference(\"i missed you\")\n",
    "inference(\"Fuck you\")\n",
    "inference(\"i'm sad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77da21",
   "metadata": {},
   "source": [
    "# selected task: offensive\n",
    "\n",
    "task: classificare un tweet come offensivo o meno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cc7e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:27:52.586683Z",
     "iopub.status.busy": "2024-01-10T12:27:52.586479Z",
     "iopub.status.idle": "2024-01-10T12:27:57.496155Z",
     "shell.execute_reply": "2024-01-10T12:27:57.494516Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tweet_eval\", \"offensive\")\n",
    "print(dataset)\n",
    "id2label = {0: \"non offensive\", 1: \"offensive\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de660f",
   "metadata": {},
   "source": [
    "Tokenizer & Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d3bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:27:57.503482Z",
     "iopub.status.busy": "2024-01-10T12:27:57.502131Z",
     "iopub.status.idle": "2024-01-10T12:53:54.133043Z",
     "shell.execute_reply": "2024-01-10T12:53:54.131991Z"
    }
   },
   "outputs": [],
   "source": [
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name_model_pretrained)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "tokenized_tweet_eval = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    name_model_pretrained, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./text_classification_model/offensive/\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    # lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    optim=\"adamw_torch\"\n",
    "    # no_cuda=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_tweet_eval[\"train\"],\n",
    "    eval_dataset=tokenized_tweet_eval[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490254f4",
   "metadata": {},
   "source": [
    "In questo caso si sta manifestando il fenomeno dell'overfitting ma la performance è ottima sul validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f0f72",
   "metadata": {},
   "source": [
    "Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17443e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:53:54.138448Z",
     "iopub.status.busy": "2024-01-10T12:53:54.138258Z",
     "iopub.status.idle": "2024-01-10T12:54:04.514237Z",
     "shell.execute_reply": "2024-01-10T12:54:04.512826Z"
    }
   },
   "outputs": [],
   "source": [
    "path_models = \"./text_classification_model/offensive/best_model/\"\n",
    "trainer.save_model(path_models)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path_models)\n",
    "trainer.evaluate(eval_dataset=tokenized_tweet_eval[\"test\"])  # on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51ad7e",
   "metadata": {},
   "source": [
    "Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797494d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T12:54:04.519747Z",
     "iopub.status.busy": "2024-01-10T12:54:04.519128Z",
     "iopub.status.idle": "2024-01-10T12:54:27.055396Z",
     "shell.execute_reply": "2024-01-10T12:54:27.053654Z"
    }
   },
   "outputs": [],
   "source": [
    "inference(\"You're so stupid.\")\n",
    "inference(\"I appreciate your hard work.\")\n",
    "inference(\"You are a wonderful person.\")\n",
    "inference(\"You look ugly today.\")\n",
    "inference(\"I can't believe you did that, you're amazing!\")\n",
    "inference(\"You're a worthless human being.\")\n",
    "inference(\"You're such a kind soul.\")\n",
    "inference(\"You're a terrible friend.\")\n",
    "inference(\"You're an inspiration to others.\")\n",
    "inference(\"You're a waste of space.\")\n",
    "inference(\"you are a disgrace\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
