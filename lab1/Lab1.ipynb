{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1faedc",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "Questo laboratorio è stato implementato usando il framework PyTorch. Ho sfruttato questo lavoro per fare pratica con il framework, provare Weights & Bias e soprattutto per cercare di scrivere codice python per fare degli esperimenti che siano indipendenti da vari fattori. L'obiettivo iniziale era quello di costruire delle architetture che siano indipendenti dal dataset usato ma vederemo che i modelli hanno dei vincoli sulla loro struttura che in certi casi vanno in conflitto con la possibilità di astrazione dello stesso.\n",
    "Ho sfruttato wandb per dare la possibilità all'utente utilizzatore del codice di poter interagire con le metriche di performance risultanti dopo l'addestramento (soprattutto attraverso semplici grafici, matrici di confusione, ecc..)\n",
    "\n",
    "Nel notebook ho lasciato anche delle porzioni di codice che \"non funzionano\" e di cui si può discutere in fase di esame.\n",
    "\n",
    "Nota bene: questo notebook è scritto nello stile di un file python puro e l'output di ogni cella corrisponde all'esprimento corrente che si vuole eseguire. Tutti gli esperimenti fatti e i loro risultati è possibile trovarli su wandb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1d54a",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">\n",
    "\n",
    "## Link esperimenti svolti: [Link wandb](https://wandb.ai/team-vittoriocasula/Lab1_DLA/reports/Esperimenti-Lab-1-DLA--Vmlldzo2Mjk1NDE3?accessToken=b45328mwbbpm714g3hkcefwgcjsd6tthd9klf0clfh3p5rr0csqs00zzu5y6xkg8)\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "## Exercise 1: Warming Up\n",
    "\n",
    "In this series of exercises I want you to try to duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "We will do this in steps using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "Recall that the main message of the ResNet paper is that **deeper** networks do not **guarantee** more reduction in training loss (or in validation accuracy). Below you will incrementally build a sequence of experiments to verify this for an MLP.\n",
    "\n",
    "### Exercise 1.1: A baseline MLP <span style=\"color: red;\">(DONE)</span>\n",
    "\n",
    "Implement a _simple_ Multilayer Perceptron to classify the 10 digits of MNIST (e.g. two _narrow_ layers). Use my code above as inspiration, but implement your own training pipeline -- you will need it later. Train this model to convergence, monitoring (at least) the loss and accuracy on the training and validation sets for every epoch. Below I include a basic implementation to get you started -- remember that you should write your _own_ pipeline!\n",
    "\n",
    "**Note**: This would be a good time to think about _abstracting_ your model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models.\n",
    "\n",
    "### Exercise 1.2: Rinse and Repeat <span style=\"color: red;\">(DONE)</span>\n",
    "\n",
    "Repeat the verification you did above, but with **Convolutional** Neural Networks. If you were careful about abstracting your model and training code, this should be a simple exercise. Show that **deeper** CNNs _without_ residual connections do not always work better and **even deeper** ones _with_ residual connections.\n",
    "\n",
    "**Hint**: You probably should do this exercise using CIFAR10, since MNIST is _very_ easy (at least up to about 99% accuracy).\n",
    "\n",
    "**Spoiler**: If you plan to do optional exercise 3.3, you should think _very_ carefully about the architectures of your CNNs here (so you can reuse them!).\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Below are **three** exercises that ask you to deepen your understanding of Deep Networks for visual recognition. You must choose **at least one** of the below for your final submission -- feel free to do **more**, but at least **ONE** you must submit.\n",
    "\n",
    "### Exercise 2.1: Explain why Residual Connections are so effective\n",
    "\n",
    "Use your two models (with and without residual connections) you developed above to study and **quantify** why the residual versions of the networks learn more effectively.\n",
    "\n",
    "**Hint**: A good starting point might be looking at the gradient magnitudes passing through the networks during backpropagation.\n",
    "\n",
    "### Exercise 2.2: Fully-convolutionalize a network.\n",
    "\n",
    "Take one of your trained classifiers and **fully-convolutionalize** it. That is, turn it into a network that can predict classification outputs at _all_ pixels in an input image. Can you turn this into a **detector** of handwritten digits? Give it a try.\n",
    "\n",
    "**Hint 1**: Sometimes the process of fully-convolutionalization is called \"network surgery\".\n",
    "\n",
    "**Hint 2**: To test your fully-convolutionalized networks you might want to write some functions to take random MNIST samples and embed them into a larger image (i.e. in a regular grid or at random positions).\n",
    "\n",
    "### Exercise 2.3: _Explain_ the predictions of a CNN <span style=\"color: red;\">(DONE)</span>\n",
    "\n",
    "Use the CNN model you trained in Exercise 1.2 and implement [_Class Activation Maps_](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n",
    "\n",
    "> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "\n",
    "Use your implementation to demonstrate how your trained CNN _attends_ to specific image features to recognize _specific_ classes.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a41dba",
   "metadata": {},
   "source": [
    "> ### Installation\n",
    "\n",
    "Run the following instruction to create a conda env:\n",
    "\n",
    "- conda create --name Lab1_DLA\n",
    "- conda activate Lab1_DLA\n",
    "- conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "- pip install torchviz\n",
    "- pip install torchview\n",
    "- pip install -U scikit-learn\n",
    "- conda install matplotlib\n",
    "- conda install anaconda::pandas\n",
    "- conda install conda-forge::tqdm\n",
    "- conda install conda-forge::wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549caeca",
   "metadata": {},
   "source": [
    "> ### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a8282-2322-4dca-b76e-2f3863bc75fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:30.180942Z",
     "iopub.status.busy": "2023-12-23T16:57:30.180388Z",
     "iopub.status.idle": "2023-12-23T16:57:32.443591Z",
     "shell.execute_reply": "2023-12-23T16:57:32.442787Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torchviz import make_dot\n",
    "from torchview import draw_graph\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import copy\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee24025",
   "metadata": {},
   "source": [
    "> ### Device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6f84c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:32.449010Z",
     "iopub.status.busy": "2023-12-23T16:57:32.448860Z",
     "iopub.status.idle": "2023-12-23T16:57:32.508371Z",
     "shell.execute_reply": "2023-12-23T16:57:32.507579Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:\" + str(i))  # cuda:0 , cuda:1\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # yes, i have a MacBookPro with M1 Pro\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "print(\n",
    "    \"GPU: \" + torch.cuda.get_device_name(i)\n",
    "    if device == torch.device(\"cuda:\" + str(i))\n",
    "    else \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207d946",
   "metadata": {},
   "source": [
    "> ### Dataset, Architetture e Ottimizzatore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queste strutture dati contengono tutti i possibili dataset, architetture, e ottimizzatori disponibili per l'esecuzione. Per scegliere uno di questi occorre andare in una cella sottostante (nel paragrafo \"Setting Experiments\") dove occorre inserire la \"chiave\" del dizionario desiderato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6fdcaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:32.513541Z",
     "iopub.status.busy": "2023-12-23T16:57:32.513395Z",
     "iopub.status.idle": "2023-12-23T16:57:32.518091Z",
     "shell.execute_reply": "2023-12-23T16:57:32.517246Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"cifar10\": {\n",
    "        \"name\": \"cifar10\",\n",
    "        \"initial_example_shape\": (3, 32, 32),\n",
    "        \"num_classes\": 10,\n",
    "    },\n",
    "    \"mnist\": {\n",
    "        \"name\": \"mnist\",\n",
    "        \"initial_example_shape\": (1, 28, 28),\n",
    "        \"num_classes\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "architectures = {\n",
    "    \"MLP\": {\"name\": \"MLP\"},\n",
    "    \"CNN\": {\"name\": \"CNN\"},\n",
    "    \"VGG\": {\"name\": \"VGG\"},\n",
    "    \"ResNet\": {\"name\": \"ResNet\"},\n",
    "    \"PlainNetwork\": {\"name\": \"PlainNetwork\"},\n",
    "    \"CNN_GAP\": {\"name\": \"CNN_GAP\"},\n",
    "}\n",
    "\n",
    "optimizers = {\"sgd\": {\"name\": \"sgd\"}, \"adam\": {\"name\": \"adam\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb9313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:32.522667Z",
     "iopub.status.busy": "2023-12-23T16:57:32.522434Z",
     "iopub.status.idle": "2023-12-23T16:57:32.527135Z",
     "shell.execute_reply": "2023-12-23T16:57:32.526325Z"
    }
   },
   "outputs": [],
   "source": [
    "# attempt to choose an optimal initial learning rate (doesn't work).\n",
    "# I try to replicate an idea of karpathy's youtube video\n",
    "# link: https://www.youtube.com/watch?v=TCH_1BHY58I&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3\n",
    "# around 45 minute\n",
    "\n",
    "\"\"\"\n",
    "dataset = MNIST(\n",
    "    root=\"../dataset/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "# steps = len(dataset)\n",
    "steps = 1000\n",
    "lre = torch.linspace(-3, 0, steps)\n",
    "lrs = 10**lre\n",
    "model = MLP(input_size=1 * 28 * 28, width=10, depth=20, output_size=10).to(\n",
    "    device\n",
    ")  # input_size, width, depth, output_size\n",
    "\n",
    "lri = []\n",
    "lossi = []\n",
    "for i in range(steps):\n",
    "    xs, ys = dataset[i]\n",
    "    xs = xs.float().to(device)\n",
    "    ys = torch.tensor([ys]).to(device)\n",
    "    logits = model(xs)\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = lrs[i]\n",
    "    for p in model.parameters():\n",
    "        p.data += -lr * p.grad\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())\n",
    "plt.plot(lri, lossi)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02067a",
   "metadata": {
    "tags": []
   },
   "source": [
    "> ## Setting Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui è possibile scegliere gli iperparametri desiderati per l'esperimento.\n",
    "Nota bene: questi non sono tutti gli iperparametri su cui è possibile sperimentare. Nella sezione \"creazione del modello\" ci sono tutti gli iperparametri relativi alle singole architetture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4174c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:32.531953Z",
     "iopub.status.busy": "2023-12-23T16:57:32.531711Z",
     "iopub.status.idle": "2023-12-23T16:57:32.536125Z",
     "shell.execute_reply": "2023-12-23T16:57:32.535281Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resize_values = (96, 96)\n",
    "dataset = data[\"cifar10\"]\n",
    "architecture = architectures[\"ResNet\"]\n",
    "optimizer = optimizers[\"sgd\"]\n",
    "batch_size = 64\n",
    "val_size = 10000\n",
    "epochs = 20\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbf13f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:32.541897Z",
     "iopub.status.busy": "2023-12-23T16:57:32.541605Z",
     "iopub.status.idle": "2023-12-23T16:57:32.548280Z",
     "shell.execute_reply": "2023-12-23T16:57:32.547336Z"
    }
   },
   "outputs": [],
   "source": [
    "lst = list(dataset[\"initial_example_shape\"])\n",
    "# la shape è una tuple, per modificarla occorre passare dalle liste\n",
    "lst[1] = resize_values[0]  # modifica le dimensioni spaziali delle immagini\n",
    "lst[2] = resize_values[1]  # modifica le dimensioni spaziali delle immagini\n",
    "example_shape_resized = tuple(lst)\n",
    "\n",
    "input_size = result = reduce(lambda a, b: a * b, example_shape_resized)  # i.e. 3*32*32\n",
    "batch_shape = (batch_size,) + example_shape_resized  # ex. (64,3,32,32)\n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"batch_shape: {batch_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970dad7",
   "metadata": {},
   "source": [
    "> ### Setting WANDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79312df0",
   "metadata": {},
   "source": [
    "Questi sono gli iperparametri da passare a wandb e che sono visualizzabili sulla piattaforma web sotto forma tabellare. In questo modo si ha una visualizzazione chiara di come questi valori cambiano al variare degli esperimenti fatti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d270090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:32.554310Z",
     "iopub.status.busy": "2023-12-23T16:57:32.554009Z",
     "iopub.status.idle": "2023-12-23T16:57:37.989828Z",
     "shell.execute_reply": "2023-12-23T16:57:37.989068Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Lab1.ipynb\"\n",
    "\n",
    "wandb.init(\n",
    "    job_type=f'{dataset[\"name\"]}',\n",
    "    name=f'{architecture[\"name\"]}',  # name of run\n",
    "    project=\"Lab1_DLA\",\n",
    "    config={\n",
    "        \"dataset\": dataset[\"name\"],\n",
    "        \"architecture\": architecture[\"name\"],\n",
    "        \"optimizers\": optimizer[\"name\"],\n",
    "        \"initial_lr\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch size\": batch_size,\n",
    "        \"resized_values\": resize_values,\n",
    "        \"val_size\": val_size,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc12cc-8422-47bf-8d8e-0950ac05ae96",
   "metadata": {},
   "source": [
    "> ### Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec0011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:37.995242Z",
     "iopub.status.busy": "2023-12-23T16:57:37.995075Z",
     "iopub.status.idle": "2023-12-23T16:57:38.003280Z",
     "shell.execute_reply": "2023-12-23T16:57:38.002557Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot label distribution with numers (MNIST style)\n",
    "def plot_hist_labels_distribution(data, num_classes, split, position):\n",
    "    if split == \"train\":\n",
    "        all_labels = torch.tensor(\n",
    "            [tensor.item() for tensor in list(Counter(data.dataset.targets).keys())]\n",
    "        ).float()\n",
    "    elif split == \"val\":\n",
    "        labels_val = [y for _, y in data]\n",
    "        all_labels = torch.tensor([value for value in list(labels_val)]).float()\n",
    "    elif split == \"test\":\n",
    "        all_labels = torch.tensor(\n",
    "            [tensor.item() for tensor in list(Counter(data.targets).keys())]\n",
    "        ).float()\n",
    "\n",
    "    hist = torch.histc(all_labels, bins=num_classes)\n",
    "    plt.subplot(1, 3, position)\n",
    "    plt.bar(range(num_classes), hist, align=\"center\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Number of Examples\")\n",
    "    plt.xticks(np.arange(0, num_classes, 1))\n",
    "    plt.title(split + \" \" + \"Set Label Distribution\")\n",
    "\n",
    "\n",
    "# plot label distribution with string (cifar10 style)\n",
    "\n",
    "\n",
    "def plot_hist_labels_distribution_string(data, num_classes, split, position):\n",
    "    if split == \"train\":\n",
    "        labels_train = [y for _, y in data]\n",
    "        all_labels = torch.tensor([value for value in list(labels_train)]).float()\n",
    "        classes = data.dataset.classes\n",
    "    elif split == \"val\":\n",
    "        labels_val = [y for _, y in data]\n",
    "        all_labels = torch.tensor([value for value in list(labels_val)]).float()\n",
    "        classes = data.dataset.classes\n",
    "    elif split == \"test\":\n",
    "        all_labels = torch.tensor([value for value in list(data.targets)]).float()\n",
    "        classes = data.classes\n",
    "\n",
    "    hist = torch.histc(all_labels, bins=num_classes)\n",
    "    plt.subplot(1, 3, position)\n",
    "    plt.bar(range(num_classes), hist, align=\"center\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Number of Examples\")\n",
    "    plt.xticks(np.arange(0, num_classes, 1), classes, rotation=90)\n",
    "    plt.title(split + \" \" + \"Set Label Distribution\")\n",
    "\n",
    "\n",
    "# for mnist: get_normalize_values(ds_train_initial.data.float())\n",
    "# for cifar10: get_normalize_values(ds_train_initial.data)\n",
    "\n",
    "\n",
    "def get_normalize_values(data):\n",
    "    return (\n",
    "        data.mean(axis=(0, 1, 2)) / 255,\n",
    "        data.std(axis=(0, 1, 2)) / 255,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella prossima cella vengono scaricati, creati i dataset e viene eseguita l'operazione di split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a69db-0416-444a-9be4-5f055ff48bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:38.007714Z",
     "iopub.status.busy": "2023-12-23T16:57:38.007565Z",
     "iopub.status.idle": "2023-12-23T16:57:52.333692Z",
     "shell.execute_reply": "2023-12-23T16:57:52.332314Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "match dataset[\"name\"]:\n",
    "    case \"mnist\":\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                transforms.Resize(size=resize_values, antialias=True),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        ds_train_initial = MNIST(\n",
    "            root=\"../dataset/\", train=True, download=True, transform=transform\n",
    "        )\n",
    "        ds_test = MNIST(\n",
    "            root=\"../dataset/\", train=False, download=True, transform=transform\n",
    "        )\n",
    "\n",
    "        n_examples_test = len(ds_test)\n",
    "        I = np.random.permutation(len(ds_train_initial))\n",
    "        ds_val = Subset(ds_train_initial, I[:val_size])\n",
    "        n_examples_val = len(ds_val)\n",
    "\n",
    "        ds_train = Subset(ds_train_initial, I[val_size:])\n",
    "        n_examples_train = len(ds_train)\n",
    "\n",
    "        print(f\"N° samples train: {n_examples_train}\")\n",
    "        print(f\"N° samples validation: {n_examples_val}\")\n",
    "        print(f\"N° samples test: {n_examples_test}\")\n",
    "        # Imposta la dimensione della figura per contenere i 3 grafici in riga\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plot_hist_labels_distribution(ds_train, dataset[\"num_classes\"], \"train\", 1)\n",
    "        plot_hist_labels_distribution(ds_val, dataset[\"num_classes\"], \"val\", 2)\n",
    "        plot_hist_labels_distribution(ds_test, dataset[\"num_classes\"], \"test\", 3)\n",
    "        plt.tight_layout()  # Ottimizza lo spaziamento tra i grafici\n",
    "\n",
    "    case \"cifar10\":\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    (0.49139968, 0.48215841, 0.44653091),\n",
    "                    (0.24703223, 0.24348513, 0.26158784),\n",
    "                ),\n",
    "                transforms.Resize(size=resize_values, antialias=True),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        ds_train_initial = CIFAR10(\n",
    "            root=\"../dataset/\", train=True, download=True, transform=transform\n",
    "        )\n",
    "\n",
    "        ds_test = CIFAR10(\n",
    "            root=\"../dataset/\", train=False, download=True, transform=transform\n",
    "        )\n",
    "\n",
    "        n_examples_test = len(ds_test)\n",
    "\n",
    "        print(f\"Classes: {ds_train_initial.class_to_idx}\")\n",
    "\n",
    "        I = np.random.permutation(len(ds_train_initial))\n",
    "        ds_val = Subset(ds_train_initial, I[:val_size])\n",
    "        n_examples_val = len(ds_val)\n",
    "\n",
    "        ds_train = Subset(ds_train_initial, I[val_size:])\n",
    "        n_examples_train = len(ds_train)\n",
    "\n",
    "        print(f\"N° samples train: {n_examples_train}\")\n",
    "        print(f\"N° samples validation: {n_examples_val}\")\n",
    "        print(f\"N° samples test: {n_examples_test}\")\n",
    "        # Imposta la dimensione della figura per contenere i 3 grafici in riga\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plot_hist_labels_distribution_string(\n",
    "            ds_train, dataset[\"num_classes\"], \"train\", 1\n",
    "        )\n",
    "        plot_hist_labels_distribution_string(ds_val, dataset[\"num_classes\"], \"val\", 2)\n",
    "        plot_hist_labels_distribution_string(ds_test, dataset[\"num_classes\"], \"test\", 3)\n",
    "\n",
    "        plt.tight_layout()  # Ottimizza lo spaziamento tra i grafici\n",
    "\n",
    "plt.savefig(\"img/dataset_label_distribution.png\")\n",
    "wandb.log(\n",
    "    {\"Dataset Label Distribution\": wandb.Image(\"img/dataset_label_distribution.png\")}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93baf0a5",
   "metadata": {},
   "source": [
    "> ### Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79341d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:52.339827Z",
     "iopub.status.busy": "2023-12-23T16:57:52.339562Z",
     "iopub.status.idle": "2023-12-23T16:57:52.347247Z",
     "shell.execute_reply": "2023-12-23T16:57:52.346236Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to view batch image in wandb\n",
    "def get_image_array(dataloader):\n",
    "    first_batch = (next(iter(dataloader))[0]).numpy()\n",
    "    list_batch = np.split(first_batch, len(first_batch))\n",
    "\n",
    "    if list_batch[0].shape[1] == 1:  # single channels images\n",
    "        image_array = np.array([el.squeeze() for el in list_batch])\n",
    "        images = [\n",
    "            PIL.Image.fromarray((image * 255).astype(np.uint8)) for image in image_array\n",
    "        ]\n",
    "    else:  # multiple channels images\n",
    "        image_array = np.array([el.squeeze().transpose(1, 2, 0) for el in list_batch])\n",
    "        images = [\n",
    "            PIL.Image.fromarray((image * 255).astype(\"uint8\"), \"RGB\")\n",
    "            for image in image_array\n",
    "        ]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f46f49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:52.352457Z",
     "iopub.status.busy": "2023-12-23T16:57:52.352226Z",
     "iopub.status.idle": "2023-12-23T16:57:57.020363Z",
     "shell.execute_reply": "2023-12-23T16:57:57.019454Z"
    }
   },
   "outputs": [],
   "source": [
    "# creation of batches\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size, shuffle=True, num_workers=4\n",
    ")\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size, num_workers=4)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "num_batch_train = len(dl_train)\n",
    "num_batch_val = len(dl_val)\n",
    "num_batch_test = len(dl_test)\n",
    "\n",
    "print(f\"N° batches train: {num_batch_train}\")\n",
    "print(f\"N° batches val: {num_batch_val}\")\n",
    "print(f\"N° batches test: {num_batch_test}\")\n",
    "\n",
    "# visualize in wandb a batch of images\n",
    "wandb.log(\n",
    "    {\n",
    "        \"examples train\": [wandb.Image(image) for image in get_image_array(dl_train)],\n",
    "        \"examples val\": [wandb.Image(image) for image in get_image_array(dl_val)],\n",
    "        \"examples test\": [wandb.Image(image) for image in get_image_array(dl_test)],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e05e96-7707-4490-98b8-50cb5e330af1",
   "metadata": {},
   "source": [
    "> ### Training and evaluation Epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcce348-f603-4d57-b9a8-5b1c6eba28ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:57.025865Z",
     "iopub.status.busy": "2023-12-23T16:57:57.025706Z",
     "iopub.status.idle": "2023-12-23T16:57:57.032109Z",
     "shell.execute_reply": "2023-12-23T16:57:57.031372Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to train a model for a single epoch over the data loader.\n",
    "def train_epoch(model, dl, opt, epoch, device):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for xs, ys in tqdm(dl, desc=f\"Training epoch {epoch}\", leave=True, position=0):\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(xs)\n",
    "        loss = F.cross_entropy(logits, ys)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        # accumulate values\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Function to evaluate a model for a single epoch over the data loader.\n",
    "def evaluate_model(model, dl, device, split_name=\"\"):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs, ys in tqdm(\n",
    "            dl, desc=\"\\tEvaluating on \" + split_name, leave=True, position=0\n",
    "        ):\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "\n",
    "            logits = model(xs)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            loss = F.cross_entropy(logits, ys)\n",
    "\n",
    "            # accumulate values\n",
    "            losses.append(loss.item())\n",
    "            gts.append(ys.detach().cpu().numpy())\n",
    "            predictions.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    return gts, predictions, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875008c3-306c-4e39-a845-d7bda7862621",
   "metadata": {},
   "source": [
    "> ## Architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b07d0",
   "metadata": {},
   "source": [
    "> ## MLP\n",
    "\n",
    "- input_size: dimensione delle feature in ingresso. Nel caso multidimensionale viene eseguito un flatten\n",
    "- width: numero di neuroni degli strati linear (vincolo: tutti i layer hanno la stessa width)\n",
    "- depth: numero di hidden layer\n",
    "- output_size: numero di neuroni nell'ultimo layer linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e503a-37df-4fb9-94e7-85d0adb494bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:57.036774Z",
     "iopub.status.busy": "2023-12-23T16:57:57.036632Z",
     "iopub.status.idle": "2023-12-23T16:57:57.041371Z",
     "shell.execute_reply": "2023-12-23T16:57:57.040650Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, width, depth, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, width)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(width, width) for _ in range(depth)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(width, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.input_layer(x)\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        # insert ReLU between each Linear layer\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "            out = torch.relu(out)\n",
    "\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be292d2",
   "metadata": {},
   "source": [
    "> ## CNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1a7d4",
   "metadata": {},
   "source": [
    "- input_channels: numero di canali dell'esempio in ingresso (nel caso di immagini questo è uguale a 3)\n",
    "- output_size: numero di neuroni nell'ultimo layer linear\n",
    "- depth: numero di hidden layer\n",
    "- channels_intermediate: numero di canali degli strati convoluzionali intermedi alla rete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7eaf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:57.046190Z",
     "iopub.status.busy": "2023-12-23T16:57:57.046048Z",
     "iopub.status.idle": "2023-12-23T16:57:57.053091Z",
     "shell.execute_reply": "2023-12-23T16:57:57.052376Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_size, depth, channels_intermediate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.channels_intermediate = channels_intermediate\n",
    "\n",
    "        self.kernel_size = (3, 3)\n",
    "        self.kernel_size_pool = (2, 2)\n",
    "\n",
    "        self.first_conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_channels,\n",
    "                out_channels=self.channels_intermediate,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=self.kernel_size_pool),\n",
    "        )\n",
    "\n",
    "        self.intermediate_block = nn.ModuleList()\n",
    "        for _ in range(self.depth):\n",
    "            self.intermediate_block.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=self.channels_intermediate,\n",
    "                    out_channels=self.channels_intermediate,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                )\n",
    "            )\n",
    "            self.intermediate_block.append(nn.ReLU())\n",
    "            self.intermediate_block.append(\n",
    "                nn.MaxPool2d(kernel_size=self.kernel_size_pool)\n",
    "            )\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(self._calculate_flatten_size(input_channels), 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.first_conv(x)\n",
    "\n",
    "        for layer in self.intermediate_block:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = out.flatten(1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "    def _calculate_flatten_size(self, input_channels):\n",
    "        # Calculate the size of the tensor after convolutional layers\n",
    "        fake_input = torch.randn(1, input_channels, resize_values[0], resize_values[1])\n",
    "        fake_output = self.first_conv(fake_input)\n",
    "\n",
    "        for layer in self.intermediate_block:\n",
    "            fake_output = layer(fake_output)\n",
    "\n",
    "        return fake_output.view(1, -1).size(1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.randn(64,3,32,32).to(device)\n",
    "m = CNN(input_channels= x.shape[1], output_size= 10, depth=2, channels_intermediate=20).to(device)\n",
    "res = m(x)\n",
    "print(res.shape)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95536546",
   "metadata": {},
   "source": [
    "CNN_GAP è un CNN che dopo aver eseguito una serie di convoluzioni e max pooling effettua un Global avarage pooling (GAP).\n",
    "\n",
    "Global Average Pooling is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the softmax layer.\n",
    "\n",
    "One advantage of global average pooling over the fully connected layers is that it is more native to the convolution structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6af3c",
   "metadata": {},
   "source": [
    "![gap](./img/fcVSgap.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457133ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:57.058492Z",
     "iopub.status.busy": "2023-12-23T16:57:57.058350Z",
     "iopub.status.idle": "2023-12-23T16:57:57.066599Z",
     "shell.execute_reply": "2023-12-23T16:57:57.065892Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN_GAP(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(CNN_GAP, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # Adaptive AvgPooling to any input size\n",
    "        self.fc = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def compute_cam(self, x, class_label):\n",
    "        x = x.to(device)\n",
    "        self.eval()\n",
    "        # class_weight: sono i pesi che sono collegati al neurone dell'ultimo layer corrispondente alla classe scelta\n",
    "        # class_weight.shape ex. torch.Size([10, 128])\n",
    "        class_weight = (list(self.parameters())[-2]).data\n",
    "        # channels: numero di canali dell'ultimo layer convoluzionale (canali della feature map)\n",
    "        channels = class_weight.shape[1]\n",
    "        layers = list(self.modules())\n",
    "        index_avg_poll = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            if isinstance(layer, nn.AvgPool2d) or isinstance(\n",
    "                layer, nn.AdaptiveAvgPool2d\n",
    "            ):\n",
    "                index_avg_poll = i\n",
    "        with torch.no_grad():\n",
    "            detectors = list(self.modules())[1 : -(len(layers) - index_avg_poll)]\n",
    "            # act_map.shape ex. torch.Size([64, 128, 4, 4]) (N.B. non è compreso l'avg pooling)\n",
    "            act_map = reduce(lambda x, f: f(x), detectors, x)\n",
    "        # weights: pesi relativi al neurone class_label (ex. torch.Size([128]) )\n",
    "        weights = class_weight[class_label]\n",
    "        weights = weights.view(-1, channels, 1, 1)  # (1, 128, 1, 1)\n",
    "        cam = act_map * weights  # (64, 128, 4, 4) * (1, 128, 1, 1) = (64, 128, 4, 4)\n",
    "        # cam = F.relu(cam.sum(dim=1, keepdim=True))  # (64, 1, 4, 4) #TODO insert relu or no?\n",
    "        cam = cam.sum(dim=1, keepdim=True)  # (64, 1, 4, 4)\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bcf844",
   "metadata": {},
   "source": [
    "La figura mostra i modelli che sono descritti nel paper \"Deep Residual Learning for Image Recognition\". Nei prossimi blocchi di codice ho implementato \"from scratch\" tali architetture. I dataset definiti in precedenza sono composti da immagini le cui dimensioni sono piuttosto piccole per architetture troppo profonde. Ho infatti deciso di effettuare un \"Resize\" di queste immagini ad un dimensione più alta (circa 3 volte in più) per risolvere tale problema\n",
    "\n",
    "![nets](./img/nets.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24a8af",
   "metadata": {},
   "source": [
    "Si può notare che la prima rete VGG termina con un maxpooling e 3 strati dense mentre le altre due architetture hanno un Gloabal Avarage Pooling e un solo strato dense. La prima architettura non permette quindi di calcolare la Class Activation Map (CAM) per costruzione. Le altre due invece hanno una struttura idonea all'applicazione della CAM proprio come descrive il paper ufficiale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52491c",
   "metadata": {},
   "source": [
    "> ## VGG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc1852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:57.072019Z",
     "iopub.status.busy": "2023-12-23T16:57:57.071878Z",
     "iopub.status.idle": "2023-12-23T16:57:57.076408Z",
     "shell.execute_reply": "2023-12-23T16:57:57.075664Z"
    }
   },
   "outputs": [],
   "source": [
    "VGG_types = {\n",
    "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"VGG13\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "    \"VGG16\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "    \"VGG19\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e84428",
   "metadata": {},
   "source": [
    "E' possibile instanziale le tipiche architetture VGG (mediante l'argomento di input architecture): VGG11, VGG13, VGG16 e VGG19.\n",
    "Nella variabile VGG_types è possibile vedere l'alternarsi di valori interi e stringhe. I valori interi indicano i canali degli strati convoluzionali della rete (in ordine con cui vengono applicati all'input). Le stringhe (presente solo \"M\") indicano la presenza di un MaxPool2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09924911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:57.080993Z",
     "iopub.status.busy": "2023-12-23T16:57:57.080824Z",
     "iopub.status.idle": "2023-12-23T16:57:57.088976Z",
     "shell.execute_reply": "2023-12-23T16:57:57.088226Z"
    }
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        architecture,\n",
    "        in_channels=3,\n",
    "        in_height=224,\n",
    "        in_width=224,\n",
    "        num_hidden=4096,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super(VGG, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.in_width = in_width\n",
    "        self.in_height = in_height\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_classes = num_classes\n",
    "        self.convs = self.init_convs(architecture)\n",
    "        self.fcs = self.init_fcs(architecture)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    def init_convs(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == int:\n",
    "                out_channels = x\n",
    "                layers.extend(\n",
    "                    [\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=out_channels,\n",
    "                            kernel_size=(3, 3),\n",
    "                            stride=(1, 1),\n",
    "                            padding=(1, 1),\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU(),\n",
    "                    ]\n",
    "                )\n",
    "                in_channels = x\n",
    "            else:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def init_fcs(self, architecture):\n",
    "        pool_count = architecture.count(\"M\")\n",
    "\n",
    "        factor = 2**pool_count\n",
    "\n",
    "        if (self.in_height % factor) + (self.in_width % factor) != 0:\n",
    "            raise ValueError(\n",
    "                f\"`in_height` and `in_width` must be multiples of {factor}\"\n",
    "            )\n",
    "        out_height = self.in_height // factor\n",
    "        out_width = self.in_width // factor\n",
    "\n",
    "        last_out_channels = next(x for x in architecture[::-1] if type(x) == int)\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(last_out_channels * out_height * out_width, self.num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.num_hidden, self.num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.num_hidden, self.num_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.randn(64, 3, 32, 32).to(device)\n",
    "m = VGG(\n",
    "    in_channels=3,\n",
    "    in_height=32,\n",
    "    in_width=32,\n",
    "    num_hidden=512,\n",
    "    architecture=VGG_types[\"VGG11\"],\n",
    ").to(device)\n",
    "res = m(x)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea37911",
   "metadata": {},
   "source": [
    "> ## PlainNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc149037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:57.094249Z",
     "iopub.status.busy": "2023-12-23T16:57:57.094088Z",
     "iopub.status.idle": "2023-12-23T16:57:57.107765Z",
     "shell.execute_reply": "2023-12-23T16:57:57.106989Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_individual_modules(model):\n",
    "    individual_modules = []\n",
    "\n",
    "    def collect_modules(module):\n",
    "        for layer in module.children():\n",
    "            if isinstance(layer, nn.Sequential) or isinstance(layer, SequentialBlock):\n",
    "                # Se il layer è un modulo Sequential, richiama ricorsivamente la funzione\n",
    "                collect_modules(layer)\n",
    "            else:\n",
    "                # Aggiungi il modulo singolo alla lista\n",
    "                individual_modules.append(layer)\n",
    "\n",
    "    collect_modules(model)\n",
    "\n",
    "    return individual_modules\n",
    "\n",
    "\n",
    "# CNN no residuals\n",
    "class SequentialBlock(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, out_channels, stride=1):\n",
    "        super(SequentialBlock, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=3, stride=stride, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "\n",
    "class PlainNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers, block, image_channels, num_classes=dataset[\"num_classes\"]\n",
    "    ):\n",
    "        super(PlainNetwork, self).__init__()\n",
    "\n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "        elif num_layers == 34 or num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.first_layers = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.layer1 = self.make_layers(\n",
    "            num_layers, block, layers[0], intermediate_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self.make_layers(\n",
    "            num_layers, block, layers[1], intermediate_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self.make_layers(\n",
    "            num_layers, block, layers[2], intermediate_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self.make_layers(\n",
    "            num_layers, block, layers[3], intermediate_channels=512, stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_layers(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def make_layers(\n",
    "        self, num_layers, block, num_residual_blocks, intermediate_channels, stride\n",
    "    ):\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(num_layers, self.in_channels, intermediate_channels, stride)\n",
    "        )\n",
    "        self.in_channels = intermediate_channels\n",
    "        for _ in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def compute_cam(self, x, class_label):\n",
    "        x = x.to(device)\n",
    "        self.eval()\n",
    "        # class_weight.shape --> torch.Size([10, 128])\n",
    "        # sono i pesi che sono collegati al neurone dell'ultimo layer corrispondente alla classe scelta\n",
    "        class_weight = (list(self.parameters())[-2]).data\n",
    "        channels = class_weight.shape[1]\n",
    "        # numero di canali dell'ultimo layer convoluzionale (canali della feature map)\n",
    "        layers = get_individual_modules(self)\n",
    "        index_avg_poll = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            if isinstance(layer, nn.AdaptiveAvgPool2d):\n",
    "                index_avg_poll = i\n",
    "\n",
    "        with torch.no_grad():\n",
    "            detectors = layers[0 : -(len(layers) - index_avg_poll)]\n",
    "            # act_map.shape --> torch.Size([64, 128, 4, 4])\n",
    "            act_map = reduce(lambda x, f: f(x), detectors, x)\n",
    "        # torch.Size([128]) ricavo i pesi relativi al neurone class_label\n",
    "        weights = class_weight[class_label]\n",
    "        weights = weights.view(-1, channels, 1, 1)  # (1, 128, 1, 1)\n",
    "        cam = act_map * weights  # (64, 128, 4, 4) * (1, 128, 1, 1) = (64, 128, 4, 4)\n",
    "        # cam = F.relu(cam.sum(dim=1, keepdim=True))  # (64, 1, 4, 4)\n",
    "        cam = cam.sum(dim=1, keepdim=True)  # (64, 1, 4, 4)\n",
    "        return cam\n",
    "\n",
    "\n",
    "def CNN18Plain(img_channels=batch_shape[1]):\n",
    "    return PlainNetwork(18, SequentialBlock, img_channels)\n",
    "\n",
    "\n",
    "def CNN34Plain(img_channels=batch_shape[1]):\n",
    "    return PlainNetwork(34, SequentialBlock, img_channels)\n",
    "\n",
    "\n",
    "def CNN50Plain(img_channels=batch_shape[1]):\n",
    "    return PlainNetwork(50, SequentialBlock, img_channels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.randn(64, 3, 32, 32).to(device)\n",
    "model = CNN34Plain(3, 10).to(device)\n",
    "res = model(x)\n",
    "print(res.shape)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "class_target = 3\n",
    "x, y = next(iter(dl_train))\n",
    "model = CNN34Plain(batch_shape[1], dataset[\"num_classes\"]).to(device)\n",
    "model.load_state_dict(torch.load(path_models))\n",
    "print(*get_individual_modules(model), sep=\"\\n\")\n",
    "cam = model.compute_cam(x, class_target)\n",
    "\"\"\"\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb8a482",
   "metadata": {},
   "source": [
    "> ## ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a14695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:57.113406Z",
     "iopub.status.busy": "2023-12-23T16:57:57.113231Z",
     "iopub.status.idle": "2023-12-23T16:57:57.128644Z",
     "shell.execute_reply": "2023-12-23T16:57:57.127872Z"
    }
   },
   "outputs": [],
   "source": [
    "class BlockResidual(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1\n",
    "    ):\n",
    "        super(BlockResidual, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers, block, image_channels, num_classes=dataset[\"num_classes\"]\n",
    "    ):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "        elif num_layers == 34 or num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNetLayers\n",
    "        self.layer1 = self.make_layers(\n",
    "            num_layers, block, layers[0], intermediate_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self.make_layers(\n",
    "            num_layers, block, layers[1], intermediate_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self.make_layers(\n",
    "            num_layers, block, layers[2], intermediate_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self.make_layers(\n",
    "            num_layers, block, layers[3], intermediate_channels=512, stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def make_layers(\n",
    "        self, num_layers, block, num_residual_blocks, intermediate_channels, stride\n",
    "    ):\n",
    "        layers = []\n",
    "        identity_downsample = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                self.in_channels, intermediate_channels, kernel_size=1, stride=stride\n",
    "            ),\n",
    "            nn.BatchNorm2d(intermediate_channels),\n",
    "        )\n",
    "\n",
    "        layers.append(\n",
    "            block(\n",
    "                num_layers,\n",
    "                self.in_channels,\n",
    "                intermediate_channels,\n",
    "                identity_downsample,\n",
    "                stride,\n",
    "            )\n",
    "        )\n",
    "        self.in_channels = intermediate_channels\n",
    "        for _ in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def compute_cam(self, x, class_label):\n",
    "        x = x.to(device)\n",
    "        self.eval()\n",
    "        class_weight = (list(self.parameters())[-2]).data\n",
    "\n",
    "        # class_weight.shape --> torch.Size([10, 128]) # sono i pesi che sono collegati al neurone dell'ultimo layer corrispondente alla classe scelta\n",
    "        channels = class_weight.shape[1]\n",
    "        # numero di canali dell'ultimo layer convoluzionale (canali della feature map)\n",
    "\n",
    "        layers = get_individual_modules(self)\n",
    "\n",
    "        index_avg_poll = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            if isinstance(layer, nn.AdaptiveAvgPool2d):\n",
    "                index_avg_poll = i\n",
    "\n",
    "        with torch.no_grad():\n",
    "            detectors = layers[0 : -(len(layers) - index_avg_poll)]\n",
    "            print(*detectors, sep=\"\\n\")\n",
    "\n",
    "            act_map = reduce(\n",
    "                lambda x, f: f(x), detectors, x\n",
    "            )  # act_map.shape --> torch.Size([64, 128, 4, 4])\n",
    "\n",
    "        weights = class_weight[\n",
    "            class_label\n",
    "        ]  # torch.Size([128]) ricavo i pesi relativi al neurone class_label\n",
    "        weights = weights.view(-1, channels, 1, 1)  # (1, 128, 1, 1)\n",
    "        cam = act_map * weights  # (64, 128, 4, 4) * (1, 128, 1, 1) = (64, 128, 4, 4)\n",
    "        cam = cam.sum(dim=1, keepdim=True)  # (64, 1, 4, 4)\n",
    "\n",
    "        # cam = F.relu(cam.sum(dim=1, keepdim=True))  # (64, 1, 4, 4)\n",
    "        return cam\n",
    "\n",
    "\n",
    "def ResNet18(img_channels=batch_shape[1]):\n",
    "    return ResNet(18, BlockResidual, img_channels)\n",
    "\n",
    "\n",
    "def ResNet34(img_channels=batch_shape[1]):\n",
    "    return ResNet(34, BlockResidual, img_channels)\n",
    "\n",
    "\n",
    "def ResNet50(img_channels=batch_shape[1]):\n",
    "    return ResNet(50, BlockResidual, img_channels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.randn(64, 3, 32, 32).to(device)\n",
    "model = ResNet34(3, 10).to(device)\n",
    "res = model(x)\n",
    "print(res.shape)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06e26-8fa3-414e-a502-8d1c18ba9eb7",
   "metadata": {},
   "source": [
    "> ## Creazione del modello\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cella sottostante ha il compito di creare il modello selezionato tra gli iperparametri. Si può notare che la creazione coinvolge una struttura \"switch-case\" dove in ogni case sono presenti gli iperparametri della specifica architettura. Tali iperparametri vengono aggiunti anche su wandb al fine di sperimentare diversi valori degli stessi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b57a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:57.134077Z",
     "iopub.status.busy": "2023-12-23T16:57:57.133898Z",
     "iopub.status.idle": "2023-12-23T16:57:58.481739Z",
     "shell.execute_reply": "2023-12-23T16:57:58.480877Z"
    }
   },
   "outputs": [],
   "source": [
    "match architecture[\"name\"]:\n",
    "    case \"MLP\":\n",
    "        width = 64\n",
    "        depth = 8\n",
    "\n",
    "        wandb.config.update({\"width\": width})\n",
    "        wandb.config.update({\"depth\": depth})\n",
    "\n",
    "        model = MLP(\n",
    "            input_size, width=width, depth=depth, output_size=dataset[\"num_classes\"]\n",
    "        )\n",
    "    case \"CNN\":\n",
    "        depth = 2\n",
    "        channels_intermediate = 32\n",
    "\n",
    "        wandb.config.update({\"depth\": depth})\n",
    "        wandb.config.update({\"channels_intermediate\": channels_intermediate})\n",
    "\n",
    "        model = CNN(\n",
    "            input_channels=batch_shape[1],\n",
    "            output_size=dataset[\"num_classes\"],\n",
    "            depth=depth,\n",
    "            channels_intermediate=channels_intermediate,\n",
    "        )\n",
    "    case \"CNN_GAP\":\n",
    "        model = CNN_GAP(\n",
    "            input_channels=batch_shape[1], output_size=dataset[\"num_classes\"]\n",
    "        )\n",
    "    case \"VGG\":\n",
    "        # type_vgg available: \"VGG11\", \"VGG13\", \"VGG16\", \"VGG19\"\n",
    "        type_vgg = \"VGG19\"\n",
    "        wandb.config.update({\"type_vgg\": type_vgg})\n",
    "\n",
    "        model = VGG(\n",
    "            in_channels=batch_shape[1],\n",
    "            in_height=batch_shape[2],\n",
    "            in_width=batch_shape[3],\n",
    "            architecture=VGG_types[type_vgg],\n",
    "        )\n",
    "    case \"PlainNetwork\":\n",
    "        # depth available: \"18\", \"34\", \"50\"\n",
    "        depth = \"50\"\n",
    "        wandb.config.update({\"depth\": depth})\n",
    "\n",
    "        match depth:\n",
    "            case \"18\":\n",
    "                model = CNN18Plain(batch_shape[1])\n",
    "            case \"34\":\n",
    "                model = CNN34Plain(batch_shape[1])\n",
    "            case \"50\":\n",
    "                model = CNN50Plain(batch_shape[1])\n",
    "    case \"ResNet\":\n",
    "        # depth available: \"18\", \"34\", \"50\"\n",
    "        depth = \"50\"\n",
    "        wandb.config.update({\"depth\": depth})\n",
    "\n",
    "        match depth:\n",
    "            case \"18\":\n",
    "                model = ResNet18(batch_shape[1])\n",
    "            case \"34\":\n",
    "                model = ResNet34(batch_shape[1])\n",
    "            case \"50\":\n",
    "                model = ResNet50(batch_shape[1])\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'Architecture name: {architecture[\"name\"]}')\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "print(f\"N° of parameters: {params}\")\n",
    "wandb.config.update({\"params\": params})\n",
    "\n",
    "print(f\"Model is in {next(model.parameters()).device}\")\n",
    "print(\"\\n\\nLayers of the network sorted:\\n\")\n",
    "print(*get_individual_modules(model), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91360aad",
   "metadata": {},
   "source": [
    "> ## Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe378b",
   "metadata": {},
   "source": [
    "In questa sezione è possibile variare l'ottimizzatore e i suoi iperparametri. Il learning rate iniziale è fisso ma questo varia durante l'addestramento attraverso uno strumento chiamato scheduler.\n",
    "\n",
    "In generale il lr può essere variato in base al numero di epoche oppure alle performance su un validation set. In genere il lr si vuole decrementare e non incrementare ma dipende molto dal problema che stiamo affrontando.\n",
    "\n",
    "In questo lavoro è stato scelto uno scheduler che monitora il valore della validation accuracy e se questa rimane invariata per più di 5 epochee viene decrementato il lr. Nelle reti più profonde questo strumento è fondamentale dato che scegliere il lr di partenza è sempre una scelta critica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ee460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:58.487679Z",
     "iopub.status.busy": "2023-12-23T16:57:58.487496Z",
     "iopub.status.idle": "2023-12-23T16:57:58.493849Z",
     "shell.execute_reply": "2023-12-23T16:57:58.493100Z"
    }
   },
   "outputs": [],
   "source": [
    "match optimizer[\"name\"]:\n",
    "    case \"adam\":\n",
    "        # betas, eps, weight_decay\n",
    "        opt = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    case \"sgd\":\n",
    "        # momentum, weight_decay\n",
    "        opt = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "\n",
    "sched = lr_scheduler.ReduceLROnPlateau(optimizer=opt, mode=\"min\", patience=5)\n",
    "print(f'Optimizer: {optimizer[\"name\"]} \\n\\n{opt.state_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea4c6b",
   "metadata": {},
   "source": [
    "## Visualization of pytorch models in the form of visual graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e2ed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:58.499497Z",
     "iopub.status.busy": "2023-12-23T16:57:58.499351Z",
     "iopub.status.idle": "2023-12-23T16:57:58.976896Z",
     "shell.execute_reply": "2023-12-23T16:57:58.975870Z"
    }
   },
   "outputs": [],
   "source": [
    "model_copy = copy.deepcopy(model)\n",
    "model_graph = draw_graph(model_copy, input_size=batch_shape, device=\"meta\")\n",
    "image = model_graph.visual_graph\n",
    "image.save(\"img/visual_graph\")\n",
    "wandb.log({\"Model Visual Graph\": wandb.Image(image.render(format=\"png\", cleanup=True))})\n",
    "# image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55610f21",
   "metadata": {},
   "source": [
    "## Computational Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b68a2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:57:58.986225Z",
     "iopub.status.busy": "2023-12-23T16:57:58.985668Z",
     "iopub.status.idle": "2023-12-23T16:58:08.893982Z",
     "shell.execute_reply": "2023-12-23T16:58:08.893160Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(batch_shape).to(device)\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 123626472\n",
    "\n",
    "out = model(input_tensor)\n",
    "graph = make_dot(\n",
    "    out, params=dict(model.named_parameters()), show_attrs=True, show_saved=True\n",
    ")\n",
    "graph.save(\"img/computational_graph\")\n",
    "wandb.log(\n",
    "    {\"Computational Graph\": wandb.Image(graph.render(format=\"png\", cleanup=True))}\n",
    ")\n",
    "# graph # decommentare per vedere il grafo computazionale (enorme per reti molto profonde)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e3f4e",
   "metadata": {},
   "source": [
    "> ## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24aa3a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:58:08.899394Z",
     "iopub.status.busy": "2023-12-23T16:58:08.899233Z",
     "iopub.status.idle": "2023-12-23T16:58:08.905788Z",
     "shell.execute_reply": "2023-12-23T16:58:08.905070Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_gradients_magnitudes(model, epoch):\n",
    "    # loss.backward REQUIRED\n",
    "    gradient_magnitudes = []\n",
    "    names_layers = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and \"bias\" not in name:\n",
    "            gradient_magnitudes.append(param.grad.norm().item())\n",
    "            names_layers.append(name)\n",
    "\n",
    "    # Plot dei gradienti\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(len(gradient_magnitudes)), gradient_magnitudes)\n",
    "    plt.xlabel(\"Parameters Layers\")\n",
    "    plt.ylabel(\"Gradient Magnitude\")\n",
    "    plt.title(\"Epoch: \" + str(epoch) + \" - Gradient Magnitudes of Model Parameters\")\n",
    "\n",
    "    dir_path = \"gradient_magnitudes/\" + architecture[\"name\"]\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    plt_path = dir_path + \"/gradient_magnitudes_epoch_\" + str(epoch) + \".png\"\n",
    "\n",
    "    if os.path.exists(plt_path):\n",
    "        # Se il file esiste, svuota la cartella\n",
    "        for root, dirs, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "    plt.savefig(plt_path)\n",
    "    plt.close()\n",
    "    data = [\n",
    "        [name, magnitude] for name, magnitude in zip(names_layers, gradient_magnitudes)\n",
    "    ]\n",
    "    table = wandb.Table(data=data, columns=[\"Parameter\", \"Gradient Magnitude\"])\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            f\"Gradient Magnitudes Epoch {epoch}\": wandb.plot.bar(\n",
    "                table,\n",
    "                \"Parameter\",\n",
    "                \"Gradient Magnitude\",\n",
    "                title=\"Gradient Magnitudes of Model Parameters - Epoch \" + str(epoch),\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # log_data = {f\"Gradient_Magnitudes_Epoch_{epoch}\": data}\n",
    "    # wandb.log(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63906faf",
   "metadata": {},
   "source": [
    "La cella successiva ha il compito di fare l'addestramento, valutare il modello su un validation set e salvare il modello risultante in una cartella chiamata \"model\". Durante il training per ogni epoca viene calcolata la magnitudine dei gradienti per ogni layer con parametri e sarà visualizzabile successivamente in una cella successiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89e48f-d8f3-4122-842d-1ff389499854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T16:58:08.910034Z",
     "iopub.status.busy": "2023-12-23T16:58:08.909896Z",
     "iopub.status.idle": "2023-12-23T17:09:32.533811Z",
     "shell.execute_reply": "2023-12-23T17:09:32.532708Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"./models/\" + architecture[\"name\"], exist_ok=True)\n",
    "\n",
    "path_models = (\n",
    "    \"./models/\"\n",
    "    + architecture[\"name\"]\n",
    "    + \"/model_\"\n",
    "    + dataset[\"name\"]\n",
    "    + \"_\"\n",
    "    + architecture[\"name\"]\n",
    "    + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    + \".pth\"\n",
    ")\n",
    "\n",
    "wandb.config.update({\"path_models\": path_models})\n",
    "\n",
    "if not os.path.exists(path_models):  # se non esiste fai l'addestramento\n",
    "    for epoch in range(epochs):\n",
    "        losses_train = train_epoch(model, dl_train, opt, epoch, device=device)\n",
    "\n",
    "        plot_gradients_magnitudes(model, epoch)\n",
    "\n",
    "        gts, predictions, losses_val = evaluate_model(\n",
    "            model, dl_val, device=device, split_name=\"val\"\n",
    "        )\n",
    "\n",
    "        loss_val = np.mean(losses_val)\n",
    "        sched.step(loss_val)\n",
    "        val_acc = accuracy_score(np.hstack(gts), np.hstack(predictions))\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"accuracy validation\": val_acc,\n",
    "                \"loss training\": np.mean(losses_train),\n",
    "                \"loss validation:\": loss_val,\n",
    "            }\n",
    "        )\n",
    "    torch.save(model.state_dict(), path_models)\n",
    "else:\n",
    "    print(f\"Model exist on path {path_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcad67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T17:09:32.540175Z",
     "iopub.status.busy": "2023-12-23T17:09:32.539985Z",
     "iopub.status.idle": "2023-12-23T17:09:34.938954Z",
     "shell.execute_reply": "2023-12-23T17:09:34.937680Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./gradient_magnitudes/\" + architecture[\"name\"]\n",
    "\n",
    "immagini = os.listdir(path)\n",
    "\n",
    "num_colonne = 3  # Imposta il numero di colonne desiderato\n",
    "num_immagini = len(immagini)\n",
    "num_righe = -(-num_immagini // num_colonne)  # Calcola il numero di righe necessario\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    num_righe, num_colonne, figsize=(num_colonne * 5, num_righe * 5)\n",
    ")\n",
    "\n",
    "for i, img in enumerate(immagini):\n",
    "    riga = i // num_colonne\n",
    "    colonna = i % num_colonne\n",
    "    image_path = os.path.join(path, img)\n",
    "    img = Image.open(image_path)\n",
    "    axes[riga, colonna].imshow(img)\n",
    "    axes[riga, colonna].axis(\"off\")  # Nasconde i valori degli assi\n",
    "\n",
    "# Nasconde gli assi per gli eventuali assi non utilizzati\n",
    "for r in range(num_righe):\n",
    "    for c in range(num_colonne):\n",
    "        if r * num_colonne + c >= num_immagini:\n",
    "            axes[r, c].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()  # Impedisce sovrapposizioni di layout\n",
    "plt.savefig(\"img/gradient_magnitude.png\")\n",
    "\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if param.grad is not None and \"bias\" not in name:\n",
    "        print(f\"{i} --> {name}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf0c2d",
   "metadata": {},
   "source": [
    "> ## Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b91a75",
   "metadata": {},
   "source": [
    "Evaluation sul test set (stessa funzione utilizzata per il validation set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b49db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T17:09:34.947623Z",
     "iopub.status.busy": "2023-12-23T17:09:34.947412Z",
     "iopub.status.idle": "2023-12-23T17:09:39.085688Z",
     "shell.execute_reply": "2023-12-23T17:09:39.084650Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(path_models, map_location=device))\n",
    "gts, predictions, losses_val = evaluate_model(\n",
    "    model, dl_test, device=device, split_name=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284b105",
   "metadata": {},
   "source": [
    "In questa parte viene visualizzato il classification report e calcolata l'accuracy media (alla fine dell'addestramento).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44cfa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T17:09:39.091275Z",
     "iopub.status.busy": "2023-12-23T17:09:39.091043Z",
     "iopub.status.idle": "2023-12-23T17:09:39.114591Z",
     "shell.execute_reply": "2023-12-23T17:09:39.113517Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_val = np.mean(losses_val)\n",
    "val_acc = accuracy_score(np.hstack(gts), np.hstack(predictions))\n",
    "report_acc = classification_report(\n",
    "    np.hstack(gts), np.hstack(predictions), zero_division=0, digits=3\n",
    ")\n",
    "print(f\"Accuracy report on TEST:\\n {report_acc}\")\n",
    "print(f\"\\nAccuracy at {epochs} epoch on TEST:\\n {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfceffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T17:09:39.119455Z",
     "iopub.status.busy": "2023-12-23T17:09:39.119254Z",
     "iopub.status.idle": "2023-12-23T17:09:39.400400Z",
     "shell.execute_reply": "2023-12-23T17:09:39.399372Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualizza il classification report su wandb\n",
    "df = pd.DataFrame.from_dict(\n",
    "    classification_report(\n",
    "        np.hstack(gts),\n",
    "        np.hstack(predictions),\n",
    "        zero_division=0,\n",
    "        digits=3,\n",
    "        output_dict=True,\n",
    "    )\n",
    ").transpose()\n",
    "df_label = df[:10]\n",
    "df_label.insert(0, \"class_label\", ds_test.classes)\n",
    "df_label.loc[:, \"support\"] = df_label[\"support\"].astype(int)\n",
    "\n",
    "wandb.log({\"Classification report\": wandb.Table(dataframe=df_label)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creazione della matrice di confuzione e visualizzazione su wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368145a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T17:09:39.407572Z",
     "iopub.status.busy": "2023-12-23T17:09:39.407277Z",
     "iopub.status.idle": "2023-12-23T17:09:39.697575Z",
     "shell.execute_reply": "2023-12-23T17:09:39.696392Z"
    }
   },
   "outputs": [],
   "source": [
    "all_gts = [item for sublist in gts for item in sublist]\n",
    "all_predictions = [item for sublist in predictions for item in sublist]\n",
    "\n",
    "print(\n",
    "    f\"Confusione Matrix Test Set\\n{confusion_matrix(all_gts, all_predictions, labels=torch.arange(0,10))}\"\n",
    ")\n",
    "\n",
    "wandb.log(\n",
    "    {\n",
    "        \"conf_mat\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=all_gts,\n",
    "            preds=all_predictions,\n",
    "            class_names=ds_train_initial.classes,\n",
    "            title=\"Confusion Matrix Test Set\",\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243f811-8227-4c6f-b07f-56e8cd91643a",
   "metadata": {},
   "source": [
    "> ## Class Activation Mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6342267",
   "metadata": {},
   "source": [
    "Questa funzione prende un batch di immagini e una class label e stampa le immagini e la classi activation mapping di solo le immagini con etichetta class label.\n",
    "Questa operazione viene ripetuta per ogni class label.\n",
    "Per calcolare la CAM occorre:\n",
    "\n",
    "- la classe target\n",
    "- i pesi dell'ultimo layer collegati alla classe target\n",
    "- la feature map in uscita dall'ultimo layer convoluzionale (in particolare il numero dei canali della feature map)\n",
    "\n",
    "![cams](./img/cam.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78f022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T17:09:39.714546Z",
     "iopub.status.busy": "2023-12-23T17:09:39.713741Z",
     "iopub.status.idle": "2023-12-23T17:09:39.728456Z",
     "shell.execute_reply": "2023-12-23T17:09:39.727352Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cam_with_images(model, x, class_target, n_img, height=32, width=32):\n",
    "    class_label = {v: k for k, v in ds_train_initial.class_to_idx.items()}\n",
    "    resize = transforms.Resize(size=(height, width), antialias=None)\n",
    "    img = resize(x)\n",
    "    # print(cam.shape) # (n°images, 1,4,4)\n",
    "    cam = model.compute_cam(x, class_target)\n",
    "    class_target = [class_target] * len(x)\n",
    "    probs = F.softmax(model(x), dim=1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, n_img, figsize=(24, 8), squeeze=False)\n",
    "\n",
    "    save_dir = f\"./cam_images/{dataset['name']}/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for b in range(n_img):\n",
    "        heatmap = cam[b].detach().cpu()\n",
    "        heatmap = resize(heatmap).permute(1, 2, 0)\n",
    "        heatmap = torch.clamp(heatmap, min=0)  # Imposta a 0 i valori negativi\n",
    "        heatmap = heatmap / heatmap.max()\n",
    "        # Normalizza tra 0 e 1 dividendo per il massimo valore\n",
    "        # Plot original image\n",
    "        img_display = (img[b].permute(1, 2, 0).cpu() + 1) / 2\n",
    "        # Normalize image to [0, 1]\n",
    "        img_display = torch.clamp(img_display, min=0, max=1)\n",
    "        ax[0, b].imshow(img_display)\n",
    "        # Plot CAM over the original image\n",
    "        img_display_heatmap = img_display * 0.5 + heatmap * 0.5\n",
    "        # Blend heatmap with image\n",
    "        ax[1, b].imshow(img_display_heatmap)\n",
    "        ax[1, b].imshow(heatmap, alpha=0.5, cmap=\"jet\")  # Plot normalized heatmap\n",
    "        ax[1, b].set_xlabel(\n",
    "            f\"Pr({class_label[class_target[b]]})={probs[b][class_target[b]].item()*100:.2f}%\",\n",
    "            color=\"black\",\n",
    "        )\n",
    "    img_name = f\"{class_target[0]}_image.png\"\n",
    "    img_path = os.path.join(save_dir, img_name)\n",
    "    plt.savefig(img_path)\n",
    "    wandb.log({f\"Sample {dataset['name']}_{class_target[0]}\": wandb.Image(img_path)})\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce93eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T17:09:39.734732Z",
     "iopub.status.busy": "2023-12-23T17:09:39.733813Z",
     "iopub.status.idle": "2023-12-23T17:09:58.959086Z",
     "shell.execute_reply": "2023-12-23T17:09:58.958000Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"./cam_images/\", exist_ok=True)\n",
    "\n",
    "\"\"\"\n",
    "assert (\n",
    "    architecture[\"name\"] == \"CNN_GAP\"\n",
    "    or architecture[\"name\"] == \"PlainNetwork\"\n",
    "    or architecture[\"name\"] == \"ResNet\"\n",
    "), \"L'architettura non corrisponde a CNN_GAP, PlainNetwork o ResNet.\"\n",
    "\"\"\"\n",
    "\n",
    "if (\n",
    "    architecture[\"name\"] == \"CNN_GAP\"\n",
    "    or architecture[\"name\"] == \"PlainNetwork\"\n",
    "    or architecture[\"name\"] == \"ResNet\"\n",
    "):\n",
    "    model.load_state_dict(torch.load(path_models))\n",
    "    print(f'Model: {architecture[\"name\"]}\\n')\n",
    "    x, y = next(iter(dl_test))  # retrieve first batch of my dataloader\n",
    "    label_targets = torch.arange(0, dataset[\"num_classes\"], 1).tolist()  # labels\n",
    "    class_label = {v: k for k, v in ds_train_initial.class_to_idx.items()}\n",
    "    for label_target in label_targets:\n",
    "        imgs_by_label = []\n",
    "        for img, label in zip(x, y):\n",
    "            if label.item() == label_target:\n",
    "                imgs_by_label.append(img)\n",
    "        print(f\"Img with target {class_label[label_target]}: {len(imgs_by_label)}\")\n",
    "\n",
    "        imgs_by_label = torch.stack(imgs_by_label).to(device)\n",
    "\n",
    "        plot_cam_with_images(\n",
    "            model, imgs_by_label, class_target=label_target, n_img=len(imgs_by_label)\n",
    "        )\n",
    "\n",
    "else:\n",
    "    print(\"L'architettura non corrisponde a CNN_GAP, PlainNetwork o ResNet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a5788",
   "metadata": {},
   "source": [
    "Le architetture devono avere necessariamente un avarage pooling dopo gli strati convoluzionali per poter applicare la CAM.\n",
    "In pytorch puoi usare avarage pooling 2d specificando le dimensioni del kernel altrimenti puoi usare Adaptive avarage pooling specificando che in uscita vuoi qualcosa di dimensioni 1x1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759549a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T17:09:58.965406Z",
     "iopub.status.busy": "2023-12-23T17:09:58.965186Z",
     "iopub.status.idle": "2023-12-23T17:10:04.997899Z",
     "shell.execute_reply": "2023-12-23T17:10:04.995724Z"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d6863",
   "metadata": {},
   "source": [
    "### - Per ogni run:\n",
    "\n",
    "- distribuzione delle label\n",
    "- modello sotto forma di visual graph\n",
    "- plot magnitudine dei gradiente per ogni epoca\n",
    "- curva di training loss e validation loss in funzione delle epoche\n",
    "- curva della validation accuracy in funzione delle epoche\n",
    "\n",
    "- evaluation:\n",
    "\n",
    "  - classification report\n",
    "  - matrice di confusione\n",
    "\n",
    "- explain the predictions\n",
    "\n",
    "  - visualizzazione di un batch delle heatmap divise per label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ecdab9",
   "metadata": {},
   "source": [
    "## Lista esperimenti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38558031",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "- mnist, resized_values = 28,28, MLP --> 2,4,6,8\n",
    "- mnist, resized_values = 28,28, CNN --> 1,2\n",
    "- mnist, resized_values = 28,28, CNN_GAP\n",
    "- mnist, resized_values = 96,96, VGG11\n",
    "- mnist, resized_values = 96,96, VGG13\n",
    "- mnist, resized_values = 96,96, VGG16\n",
    "- mnist, resized_values = 96,96, PlainNetwork18\n",
    "\n",
    "---\n",
    "\n",
    "## CIFAR10\n",
    "\n",
    "- cifar10, resized_values = 32, 32, MLP --> 2,4,6,8\n",
    "- cifar10, resized_values = 32, 32, CNN --> 1,2\n",
    "- cifar10, resized_values = 32, 32, CNN_GAP\n",
    "- cifar10, resized_values = 96,96, VGG11\n",
    "- cifar10, resized_values = 96,96, VGG16\n",
    "- cifar10, resized_values = 96,96, VGG19\n",
    "- cifar10, resized_values = 96,96, PlainNetwork18\n",
    "- cifar10, resized_values = 96,96, PlainNetwork34\n",
    "- cifar10, resized_values = 96,96, PlainNetwork50\n",
    "- cifar10, resized_values = 96,96, ResNet18\n",
    "- cifar10, resized_values = 96,96, ResNet34\n",
    "- cifar10, resized_values = 96,96, ResNet50\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052791c",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">\n",
    "\n",
    "## Link esperimenti svolti: [Link wandb](https://wandb.ai/team-vittoriocasula/Lab1_DLA/reports/Esperimenti-Lab-1-DLA--Vmlldzo2Mjk1NDE3?accessToken=b45328mwbbpm714g3hkcefwgcjsd6tthd9klf0clfh3p5rr0csqs00zzu5y6xkg8)\n",
    "\n",
    "</span>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
