{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "- Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "- Watch (and potentially _code along_) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "\n",
    "In this first exercise you will train a _small_ autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "- scarica il file\n",
    "- elimina la parte iniziale per fare il training\n",
    "- tokenizza il testo\n",
    "- crea il modello\n",
    "- training (per poche epoche)\n",
    "- guarda la loss\n",
    "- genera testo e valuta qualitativamente i risultati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create --name Lab2_DLA\n",
    "# conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "# conda install -c conda-forge transformers\n",
    "# pip install scikit-learn\n",
    "# pip install evaluate\n",
    "# pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c5cb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl23vitcas/anaconda3/envs/Lab2_DLA/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GenerationConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:\" + str(i))  # cuda:0 , cuda:1\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # yes, i have a MacBookPro with M1 Pro\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "print(\n",
    "    \"GPU: \" + torch.cuda.get_device_name(i)\n",
    "    if device == torch.device(\"cuda:\" + str(i))\n",
    "    else \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # how many indipendent sequences will be process in parallel ?\n",
    "# what is the maximum context length for predictions ?\n",
    "block_size = 256  # contect_lenght\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataset\\n\")\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(f\"lenght of dataset in characters: {len(text)}\\n\")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'chars: {\"\".join(chars)}')\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "# print(text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d67ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from character to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# encoder: take a string, output a list of integer\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# decoder: take a list of integer, output a string\n",
    "decode = lambda l: \"\".join(itos[i] for i in l)\n",
    "\n",
    "print(encode(\"Ciao Vittorio\"))\n",
    "print(decode(encode(\"Ciao Vittorio\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:200])\n",
    "# the 200 characters we looked at earlier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61949169",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f26272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_data[:10])\n",
    "print(f\"Context lenght: {block_size}\")\n",
    "\n",
    "train_data[: block_size + 1]  # chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6390bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "print(f\"Context lenght: {block_size}\")\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")\n",
    "\n",
    "    if t == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(low=0, high=len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(\"input: \")\n",
    "print(f\"{xb.shape}\\n{xb}\")\n",
    "print(\"target: \")\n",
    "print(f\"{yb.shape}\\n{yb}\")\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for b in range(batch_size):  # 0 --> 4\n",
    "    for t in range(block_size):  # 0 --> 8\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context} the target is {target}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82041f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=n_embd\n",
    "        )\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device)\n",
    "        )  # (T, C )\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B,T, vocab_size)\n",
    "        # n.b: cross entropy vuole i logits con shape (B * T, C) e non (B, T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # function to generate from the model\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size torkens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)  # logits.shape = (B * T, C)\n",
    "            # focus only in the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append samples index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of sel-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1)  # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, T) @ (B,T,C) --> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallell\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Trasformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./text_generation_model/\", exist_ok=True)\n",
    "\n",
    "path_models = \"./text_generation_model/model.pth\"\n",
    "\n",
    "if not os.path.exists(path_models):  # se non esiste fai l'addestramento\n",
    "    for iter in range(max_iters):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss(model)\n",
    "            print(\n",
    "                f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "        xb, yb = get_batch(\"train\")\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.save(model.state_dict(), path_models)\n",
    "else:\n",
    "    print(f\"Model exist on path {path_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(path_models)) # questa istruzione crea problemi se addestro su gpu su papavero e provo a caricare i pesi su mps o cpu, usa torhc.load specificando map_location il dispositivo mps o cpu\n",
    "# torch.load(path_models, map_location=device)\n",
    "\n",
    "model.load_state_dict(torch.load(path_models, map_location=device))\n",
    "\n",
    "max_new_tokens = 5000\n",
    "\n",
    "# generates text\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=max_new_tokens)[0].tolist()))\n",
    "# il testo generato a seguito dell'addestramento dovrebbe avere senso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ce8e2",
   "metadata": {},
   "source": [
    "Il testo generato ha una grande somiglianza con lo stile di scrittura del noto poeta Dante. Si può notare come le frasi siano corte e risultano suddivise in gruppi di tre frase (chiamate \"terzine\"). Ogni terzina ha le due ultime due frasi \"indentate\" a destra. La maggiorparte delle parole hanno un senso nella lingua italiana, altre invece contengono dei semplici errori di lettere all'interno.\n",
    "Altra fattore da evidenziare è che il modello spesso riesce a capire che quando siamo all'inizio di una conversazione e viene posto il carattere \":\" prima del suo inizio.\n",
    "\n",
    "Sicuramente con un addestramento con più epoche avremmo un risultato molto migliore ricco di altre caratteristiche dello stile di dante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a _huge_ variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "\n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text).\n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists).\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installa HF transformers (DONE)\n",
    "# è stato necessario eseguire anche --> conda install -c conda-forge huggingface_hub (DONE)\n",
    "# encode text into sub-word tokens with GPT2Tokenizer (DONE)\n",
    "# istanzia GPT2Tokenizer e converti il testo in token interi (DONE)\n",
    "# confronta la lunghezza dell'input (testo) con la lunghezza della sequenza codificata (lunghezza della sequenza di interi) (DONE)\n",
    "# ricordati di usare return_tensors='pt' in modo da ottenere un tensore torch (DONE)\n",
    "# usa come modello GPT2LMHeadModel (questo modello ha un'uscita apposita per generare il testo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# stampa gli attributi e i metodi che posso invocare con l'oggetto tokenizer\n",
    "# print(*dir(tokenizer), sep=\"\\n\")\n",
    "# print(tokenizer.get_vocab) # important params settings\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\\n\")\n",
    "\n",
    "print(tokenizer.encode(\"Hi Vittorio\"))  # dict with keys: input_ids, attention_mask\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hi Vittorio\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb89674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_encoded_text(text, huge=False):\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    contents = f\"{text} --> {encoded_text}\" if huge == False else \"\"\n",
    "    print(\n",
    "        f\"° length text (in characters): {len(text)} - length encoded text: {encoded_text.shape[1]}\\t {contents}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Esempi:\\n\")\n",
    "view_encoded_text(\"Good Morning!\")\n",
    "view_encoded_text(\" Good Morning!\")\n",
    "view_encoded_text(\"  Good Morning!\")\n",
    "view_encoded_text(\"   Good Morning!\")\n",
    "view_encoded_text(\"Good Morning!   \")\n",
    "view_encoded_text(\"Good Morning!  \")\n",
    "view_encoded_text(\"Good Morning! \")\n",
    "view_encoded_text(\"Good Morning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed4517",
   "metadata": {},
   "source": [
    "Da notare che la lunghezza del testo codificato tiene conto del numero di parole e della punteggiatura (carattere spazio incluso).\n",
    "Per evitare di scrivere un programma che conti il numero di parole e il numero di caratteri della punteggiatura contiamo i caratteri della testo come avevamo fatto nel precedente esercizio.\n",
    "\n",
    "Si può notare che all'aumentare/diminuire della lunghezza del testo aumenta/diminuisce la lunghezza della sequenza di interi. Però fra le due lenght non c'è una corrispondenza biunivoca (per esempio ad un testo di 13 caratteri potrebbero corrispondere una codifica con 3 o 4 interi come si può vedere negli esempi sopra).\n",
    "Si può notare inoltre che l'aggiunta di tanti spazi fa aumentare la lunghezza della sequenza di interi. Questo potrebbe essere uno svantaggio in certi casi. In genere l'aggiunta di più di uno spazio nella scrittura con tastiera qwerty è un'errore di battitura che però un modello di linguaggio codifica come input di testo grezzo e potrebbe creare delle relazioni tra parole successive.\n",
    "\n",
    "Proviamo con esempi di testo più lunghi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_encoded_text(\n",
    "    \"My name is Vittorio Casula. I'm 26 years old. I graduated in Computer Engineering in 2021 at University of Florence.\",\n",
    "    huge=True,\n",
    ")\n",
    "view_encoded_text(\n",
    "    \"My best purchase of the last 2 years is my Macbook Pro with M1 Pro Processor. Thanks to it, I'm able to run fast my Machine Learning Script on my laptop.\",\n",
    "    huge=True,\n",
    ")\n",
    "view_encoded_text(\n",
    "    \"However, I believe that the optimal setup for working in the Machine Learning field should involve two devices: the first being a laptop (not necessarily a MacBook) with a large-capacity battery, and the second being a server (connected via SSH) on which you can run your scripts without utilizing the resources of the laptop.\",\n",
    "    huge=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proviamo a fare una stringa dinamica che diventa sempre più lunga\n",
    "\n",
    "dynamic_str = \"\"\n",
    "len_max = 1024\n",
    "chars = f\"ABCDEFGHILMNOPQRSTUVXZabcdefghijlmnopqrstuvxz0123456789\"\n",
    "indices = torch.randint(low=0, high=len(chars), size=(len_max,)).tolist()\n",
    "\n",
    "lengths_text = []\n",
    "lengths_encoded_text = []\n",
    "for i in range(len(indices)):\n",
    "    dynamic_str += chars[indices[i]]\n",
    "    lengths_text.append(len(dynamic_str))\n",
    "    lengths_encoded_text.append(\n",
    "        tokenizer(dynamic_str, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    )\n",
    "\n",
    "plt.title(\"Lunghezza del testo (in caratteri) vs Lunghezza del testo codificato\")\n",
    "plt.xlabel(\"Lunghezza (in caratteri) del testo\")\n",
    "plt.ylabel(\"Lunghezza del testo codificato\")\n",
    "\n",
    "plt.plot(lengths_text, lengths_encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a _prompt_ in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is _greedy_ which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = \"My name is Vittorio. I'm AI student at University of Florence.\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "# print(*inputs)  # get the keys of returned dict\n",
    "# outputs = model.generate(**inputs) # warning: Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "# print(outputs)\n",
    "print(f\"Generated text: \\n{tokenizer.decode(outputs[0])}\")\n",
    "# if you want decode multiple interger sequence you should use batch_decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86461d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting warning:Using the model-agnostic default `max_length` (=20) to control thegeneration length.\n",
    "# We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=100\n",
    ")\n",
    "# print(outputs)\n",
    "print(\"Generated text: \\n\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0894e8",
   "metadata": {},
   "source": [
    "Come vediamo il testo generato contiene il testo iniziamente codificato e una frase generata che si ripete fino a raggiungere il numero massimo di token specificato. Non ha molto senso questo testo generato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd431678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \"do_sample\" and \"temperature\" parameters\n",
    "# link: https://huggingface.co/docs/transformers/generation_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generation_config)\n",
    "# Printing out the model.generation_config reveals only the values that are different from the default generation configuration,\n",
    "# and does not list any of the default values.\n",
    "\n",
    "# The default generation configuration limits the size of the output combined with the input prompt to a maximum of 20 tokens\n",
    "# to avoid running into resource limitations. The default decoding strategy is greedy search, which is the simplest decoding\n",
    "# strategy that picks a token with the highest probability as the next token. For many tasks and small output sizes this works well.\n",
    "# However, when used to generate longer outputs, greedy search can start producing highly repetitive results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e8b10",
   "metadata": {},
   "source": [
    "### Customize Text Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02da57",
   "metadata": {},
   "source": [
    "do_sample: if set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling,\n",
    "Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary\n",
    "with various strategy-specific adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=100, do_sample=True\n",
    ")\n",
    "# print(outputs)\n",
    "print(\"Generated text: \\n\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "# print(outputs)\n",
    "print(\"Generated text: \\n\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f75701",
   "metadata": {},
   "source": [
    "Assisted decoding is a modification of the decoding strategies above that uses an assistant model with the same tokenizer (ideally a much smaller model) to greedily generate a few candidate tokens. The main model then validates the candidate tokens in a single forward pass, which speeds up the decoding process. Currently, only greedy search and sampling are supported with assisted decoding, and doesn’t support batched inputs. To learn more about assisted decoding, check this blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0f985",
   "metadata": {},
   "source": [
    "When using assisted decoding with sampling methods, you can use the temperature argument to control the randomness just like in multinomial sampling. However, in assisted decoding, reducing the temperature will help improving latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = [0.2, 0.4, 0.6, 0.8, 1]\n",
    "for temp in temperature:\n",
    "    outputs = model.generate(\n",
    "        **inputs, generation_config=generation_config, do_sample=True, temperature=temp\n",
    "    )\n",
    "    # print(outputs)\n",
    "    print(f\"\\n° Generated text (temperature = {temp}):\")\n",
    "    print(f\"{tokenizer.decode(outputs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d5bda",
   "metadata": {},
   "source": [
    "Testo generato molto vario al crescere del parametro temperature. Per piccoli valori di temperature si ha il fenomeno della ripetizione del testo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, _at least_ one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "- Since GPT2 is a _autoregressive_ model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select _one_ to use).\n",
    "\n",
    "- BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "- The first _two_ exercises below can probably be done _without_ any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "# Exercise 3.1: Training a Text Classifier (easy) <span style=\"color: red;\">(DONE)</span>\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a _moderately_ sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM _exclusively_ as a feature extractor and then train a shallow model.\n",
    "\n",
    "# Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a _moderately_ sized one and train a model to answer contextualized multiple-choice questions. You _might_ be able to avoid fine-tuning by training a simple model to _rank_ the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure _similarity_ between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the _retrieval_ problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prendere un modello pre-addestrato e adattarlo ad un nuovo task\n",
    "# non occorre fare fine-tuning\n",
    "# addestrare uno shallow MLP per classificare o rappresentare con una loss function\n",
    "# dataset: sceglierne uno di moderate dimensioni\n",
    "# LLM: GPT2\n",
    "# usare LLM esclusivamente come feature extractor\n",
    "# addestrare uno shallow MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4d600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b7704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 2)\n",
      "(50000, 2)\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].shape)\n",
    "print(dataset[\"test\"].shape)\n",
    "print(dataset[\"validation\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5b497b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Sunday afternoon walking through Venice in the sun with @user ️ ️ ️ @ Abbot Kinney, Venice',\n",
       "  \"Time for some BBQ and whiskey libations. Chomp, belch, chomp! (@ Lucille's Smokehouse Bar-B-Que)\",\n",
       "  'Love love love all these people ️ ️ ️ #friends #bff #celebrate #blessed #sundayfunday @ San…',\n",
       "  '️ ️ ️ ️ @ Toys\"R\"Us',\n",
       "  'Man these are the funniest kids ever!! That face! #HappyBirthdayBubb @ FLIPnOUT Xtreme'],\n",
       " 'label': [12, 19, 0, 0, 2]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5314fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9\",\n",
    "    10: \"10\",\n",
    "    11: \"11\",\n",
    "    12: \"12\",\n",
    "    13: \"13\",\n",
    "    14: \"14\",\n",
    "    15: \"15\",\n",
    "    16: \"16\",\n",
    "    17: \"17\",\n",
    "    18: \"18\",\n",
    "    19: \"19\",\n",
    "}\n",
    "label2id = {\n",
    "    \"0\": 0,\n",
    "    \"1\": 1,\n",
    "    \"2\": 2,\n",
    "    \"3\": 3,\n",
    "    \"4\": 4,\n",
    "    \"5\": 5,\n",
    "    \"6\": 6,\n",
    "    \"7\": 7,\n",
    "    \"8\": 8,\n",
    "    \"9\": 9,\n",
    "    \"10\": 10,\n",
    "    \"11\": 11,\n",
    "    \"12\": 12,\n",
    "    \"13\": 13,\n",
    "    \"14\": 14,\n",
    "    \"15\": 15,\n",
    "    \"16\": 16,\n",
    "    \"17\": 17,\n",
    "    \"18\": 18,\n",
    "    \"19\": 19,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0133bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "# Puoi sostituire '[PAD]' con il token di padding desiderato\n",
    "tokenizer.pad_token = \"[PAD]\"  # Imposta il token appena aggiunto come token di padding\n",
    "\n",
    "tokenized_tweet_eval = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    \"gpt2\", num_labels=20, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779eb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt-2\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "# Puoi sostituire '[PAD]' con il token di padding desiderato\n",
    "tokenizer.pad_token = \"[PAD]\"  # Imposta il token appena aggiunto come token di padding\n",
    "\n",
    "\n",
    "tokenized_tweet_eval = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "accuracy = evaluate.load(\"accuracy\")\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7cca58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf10444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 19578.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "tokenized_tweet_eval = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e239a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"gpt2\", num_labels=20, id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36d7d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./text_classification_model/\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    optim=\"adamw_torch\"\n",
    "    # no_cuda=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_tweet_eval[\"train\"],\n",
    "    eval_dataset=tokenized_tweet_eval[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca30bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2814' max='2814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2814/2814 26:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.110300</td>\n",
       "      <td>1.945927</td>\n",
       "      <td>0.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.908900</td>\n",
       "      <td>1.873788</td>\n",
       "      <td>0.424100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl23vitcas/anaconda3/envs/Lab2_DLA/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2814, training_loss=2.0402637039759353, metrics={'train_runtime': 1565.7625, 'train_samples_per_second': 57.48, 'train_steps_per_second': 1.797, 'total_flos': 5370704682393600.0, 'train_loss': 2.0402637039759353, 'epoch': 2.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006feae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl23vitcas/anaconda3/envs/Lab2_DLA/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 02:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.8737884759902954,\n",
       " 'eval_accuracy': 0.4241,\n",
       " 'eval_runtime': 162.1446,\n",
       " 'eval_samples_per_second': 308.367,\n",
       " 'eval_steps_per_second': 9.64,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "156af079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe predetta: 17\n"
     ]
    }
   ],
   "source": [
    "path_models = \"text_classification_model\"\n",
    "\n",
    "sequence_to_classify = \"merry cristhmas\"\n",
    "\n",
    "model.save_pretrained(path_models)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path_models)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "encoded_sequence = tokenizer(sequence_to_classify, return_tensors=\"pt\")\n",
    "\n",
    "logits = model(**encoded_sequence).logits\n",
    "predicted_class = logits.argmax().item()\n",
    "\n",
    "print(f\"Classe predetta: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
