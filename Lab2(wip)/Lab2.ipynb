{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "- Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "- Watch (and potentially _code along_) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "\n",
    "In this first exercise you will train a _small_ autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "- scarica il file\n",
    "- elimina la parte iniziale per fare il training\n",
    "- tokenizza il testo\n",
    "- crea il modello\n",
    "- training (per poche epoche)\n",
    "- guarda la loss\n",
    "- genera testo e valuta qualitativamente i risultati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create --name Lab2_DLA\n",
    "# conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "# conda install -c conda-forge transformers\n",
    "# pip install scikit-learn\n",
    "# pip install evaluate\n",
    "# pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c5cb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl23vitcas/anaconda3/envs/Lab2_DLA/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GenerationConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2c5d76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "GPU: NVIDIA RTX A2000 12GB\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:\" + str(i))  # cuda:0 , cuda:1\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # yes, i have a MacBookPro with M1 Pro\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "print(\n",
    "    \"GPU: \" + torch.cuda.get_device_name(i)\n",
    "    if device == torch.device(\"cuda:\" + str(i))\n",
    "    else \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # how many indipendent sequences will be process in parallel ?\n",
    "# what is the maximum context length for predictions ?\n",
    "block_size = 256  # contect_lenght\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataset\\n\")\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(f\"lenght of dataset in characters: {len(text)}\\n\")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'chars: {\"\".join(chars)}')\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "# print(text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d67ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from character to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# encoder: take a string, output a list of integer\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# decoder: take a list of integer, output a string\n",
    "decode = lambda l: \"\".join(itos[i] for i in l)\n",
    "\n",
    "print(encode(\"Ciao Vittorio\"))\n",
    "print(decode(encode(\"Ciao Vittorio\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:200])\n",
    "# the 200 characters we looked at earlier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61949169",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f26272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_data[:10])\n",
    "print(f\"Context lenght: {block_size}\")\n",
    "\n",
    "train_data[: block_size + 1]  # chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6390bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "print(f\"Context lenght: {block_size}\")\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")\n",
    "\n",
    "    if t == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(low=0, high=len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(\"input: \")\n",
    "print(f\"{xb.shape}\\n{xb}\")\n",
    "print(\"target: \")\n",
    "print(f\"{yb.shape}\\n{yb}\")\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for b in range(batch_size):  # 0 --> 4\n",
    "    for t in range(block_size):  # 0 --> 8\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context} the target is {target}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82041f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=n_embd\n",
    "        )\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device)\n",
    "        )  # (T, C )\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B,T, vocab_size)\n",
    "        # n.b: cross entropy vuole i logits con shape (B * T, C) e non (B, T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # function to generate from the model\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size torkens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)  # logits.shape = (B * T, C)\n",
    "            # focus only in the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append samples index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of sel-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1)  # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, T) @ (B,T,C) --> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallell\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Trasformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./text_generation_model/\", exist_ok=True)\n",
    "\n",
    "path_models = \"./text_generation_model/model.pth\"\n",
    "\n",
    "if not os.path.exists(path_models):  # se non esiste fai l'addestramento\n",
    "    for iter in range(max_iters):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss(model)\n",
    "            print(\n",
    "                f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "        xb, yb = get_batch(\"train\")\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.save(model.state_dict(), path_models)\n",
    "else:\n",
    "    print(f\"Model exist on path {path_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(path_models)) # questa istruzione crea problemi se addestro su gpu su papavero e provo a caricare i pesi su mps o cpu, usa torhc.load specificando map_location il dispositivo mps o cpu\n",
    "# torch.load(path_models, map_location=device)\n",
    "\n",
    "model.load_state_dict(torch.load(path_models, map_location=device))\n",
    "\n",
    "max_new_tokens = 5000\n",
    "\n",
    "# generates text\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=max_new_tokens)[0].tolist()))\n",
    "# il testo generato a seguito dell'addestramento dovrebbe avere senso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ce8e2",
   "metadata": {},
   "source": [
    "Il testo generato ha una grande somiglianza con lo stile di scrittura del noto poeta Dante. Si può notare come le frasi siano corte e risultano suddivise in gruppi di tre frase (chiamate \"terzine\"). Ogni terzina ha le due ultime due frasi \"indentate\" a destra. La maggiorparte delle parole hanno un senso nella lingua italiana, altre invece contengono dei semplici errori di lettere all'interno.\n",
    "Altra fattore da evidenziare è che il modello spesso riesce a capire che quando siamo all'inizio di una conversazione e viene posto il carattere \":\" prima del suo inizio.\n",
    "\n",
    "Sicuramente con un addestramento con più epoche avremmo un risultato molto migliore ricco di altre caratteristiche dello stile di dante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a _huge_ variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "\n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text).\n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists).\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installa HF transformers (DONE)\n",
    "# è stato necessario eseguire anche --> conda install -c conda-forge huggingface_hub (DONE)\n",
    "# encode text into sub-word tokens with GPT2Tokenizer (DONE)\n",
    "# istanzia GPT2Tokenizer e converti il testo in token interi (DONE)\n",
    "# confronta la lunghezza dell'input (testo) con la lunghezza della sequenza codificata (lunghezza della sequenza di interi) (DONE)\n",
    "# ricordati di usare return_tensors='pt' in modo da ottenere un tensore torch (DONE)\n",
    "# usa come modello GPT2LMHeadModel (questo modello ha un'uscita apposita per generare il testo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# stampa gli attributi e i metodi che posso invocare con l'oggetto tokenizer\n",
    "# print(*dir(tokenizer), sep=\"\\n\")\n",
    "# print(tokenizer.get_vocab) # important params settings\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\\n\")\n",
    "\n",
    "print(tokenizer.encode(\"Hi Vittorio\"))  # dict with keys: input_ids, attention_mask\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hi Vittorio\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb89674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_encoded_text(text, huge=False):\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    contents = f\"{text} --> {encoded_text}\" if huge == False else \"\"\n",
    "    print(\n",
    "        f\"° length text (in characters): {len(text)} - length encoded text: {encoded_text.shape[1]}\\t {contents}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Esempi:\\n\")\n",
    "view_encoded_text(\"Good Morning!\")\n",
    "view_encoded_text(\" Good Morning!\")\n",
    "view_encoded_text(\"  Good Morning!\")\n",
    "view_encoded_text(\"   Good Morning!\")\n",
    "view_encoded_text(\"Good Morning!   \")\n",
    "view_encoded_text(\"Good Morning!  \")\n",
    "view_encoded_text(\"Good Morning! \")\n",
    "view_encoded_text(\"Good Morning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed4517",
   "metadata": {},
   "source": [
    "Da notare che la lunghezza del testo codificato tiene conto del numero di parole e della punteggiatura (carattere spazio incluso).\n",
    "Per evitare di scrivere un programma che conti il numero di parole e il numero di caratteri della punteggiatura contiamo i caratteri della testo come avevamo fatto nel precedente esercizio.\n",
    "\n",
    "Si può notare che all'aumentare/diminuire della lunghezza del testo aumenta/diminuisce la lunghezza della sequenza di interi. Però fra le due lenght non c'è una corrispondenza biunivoca (per esempio ad un testo di 13 caratteri potrebbero corrispondere una codifica con 3 o 4 interi come si può vedere negli esempi sopra).\n",
    "Si può notare inoltre che l'aggiunta di tanti spazi fa aumentare la lunghezza della sequenza di interi. Questo potrebbe essere uno svantaggio in certi casi. In genere l'aggiunta di più di uno spazio nella scrittura con tastiera qwerty è un'errore di battitura che però un modello di linguaggio codifica come input di testo grezzo e potrebbe creare delle relazioni tra parole successive.\n",
    "\n",
    "Proviamo con esempi di testo più lunghi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_encoded_text(\n",
    "    \"My name is Vittorio Casula. I'm 26 years old. I graduated in Computer Engineering in 2021 at University of Florence.\",\n",
    "    huge=True,\n",
    ")\n",
    "view_encoded_text(\n",
    "    \"My best purchase of the last 2 years is my Macbook Pro with M1 Pro Processor. Thanks to it, I'm able to run fast my Machine Learning Script on my laptop.\",\n",
    "    huge=True,\n",
    ")\n",
    "view_encoded_text(\n",
    "    \"However, I believe that the optimal setup for working in the Machine Learning field should involve two devices: the first being a laptop (not necessarily a MacBook) with a large-capacity battery, and the second being a server (connected via SSH) on which you can run your scripts without utilizing the resources of the laptop.\",\n",
    "    huge=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proviamo a fare una stringa dinamica che diventa sempre più lunga\n",
    "\n",
    "dynamic_str = \"\"\n",
    "len_max = 1024\n",
    "chars = f\"ABCDEFGHILMNOPQRSTUVXZabcdefghijlmnopqrstuvxz0123456789\"\n",
    "indices = torch.randint(low=0, high=len(chars), size=(len_max,)).tolist()\n",
    "\n",
    "lengths_text = []\n",
    "lengths_encoded_text = []\n",
    "for i in range(len(indices)):\n",
    "    dynamic_str += chars[indices[i]]\n",
    "    lengths_text.append(len(dynamic_str))\n",
    "    lengths_encoded_text.append(\n",
    "        tokenizer(dynamic_str, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    )\n",
    "\n",
    "plt.title(\"Lunghezza del testo (in caratteri) vs Lunghezza del testo codificato\")\n",
    "plt.xlabel(\"Lunghezza (in caratteri) del testo\")\n",
    "plt.ylabel(\"Lunghezza del testo codificato\")\n",
    "\n",
    "plt.plot(lengths_text, lengths_encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a _prompt_ in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is _greedy_ which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = \"My name is Vittorio. I'm AI student at University of Florence.\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "# print(*inputs)  # get the keys of returned dict\n",
    "# outputs = model.generate(**inputs) # warning: Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "# print(outputs)\n",
    "print(f\"Generated text: \\n{tokenizer.decode(outputs[0])}\")\n",
    "# if you want decode multiple interger sequence you should use batch_decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86461d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting warning:Using the model-agnostic default `max_length` (=20) to control thegeneration length.\n",
    "# We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=100\n",
    ")\n",
    "# print(outputs)\n",
    "print(\"Generated text: \\n\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0894e8",
   "metadata": {},
   "source": [
    "Come vediamo il testo generato contiene il testo iniziamente codificato e una frase generata che si ripete fino a raggiungere il numero massimo di token specificato. Non ha molto senso questo testo generato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd431678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \"do_sample\" and \"temperature\" parameters\n",
    "# link: https://huggingface.co/docs/transformers/generation_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generation_config)\n",
    "# Printing out the model.generation_config reveals only the values that are different from the default generation configuration,\n",
    "# and does not list any of the default values.\n",
    "\n",
    "# The default generation configuration limits the size of the output combined with the input prompt to a maximum of 20 tokens\n",
    "# to avoid running into resource limitations. The default decoding strategy is greedy search, which is the simplest decoding\n",
    "# strategy that picks a token with the highest probability as the next token. For many tasks and small output sizes this works well.\n",
    "# However, when used to generate longer outputs, greedy search can start producing highly repetitive results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e8b10",
   "metadata": {},
   "source": [
    "### Customize Text Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02da57",
   "metadata": {},
   "source": [
    "do_sample: if set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling,\n",
    "Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary\n",
    "with various strategy-specific adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=100, do_sample=True\n",
    ")\n",
    "# print(outputs)\n",
    "print(\"Generated text: \\n\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "# print(outputs)\n",
    "print(\"Generated text: \\n\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f75701",
   "metadata": {},
   "source": [
    "Assisted decoding is a modification of the decoding strategies above that uses an assistant model with the same tokenizer (ideally a much smaller model) to greedily generate a few candidate tokens. The main model then validates the candidate tokens in a single forward pass, which speeds up the decoding process. Currently, only greedy search and sampling are supported with assisted decoding, and doesn’t support batched inputs. To learn more about assisted decoding, check this blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0f985",
   "metadata": {},
   "source": [
    "When using assisted decoding with sampling methods, you can use the temperature argument to control the randomness just like in multinomial sampling. However, in assisted decoding, reducing the temperature will help improving latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = [0.2, 0.4, 0.6, 0.8, 1]\n",
    "for temp in temperature:\n",
    "    outputs = model.generate(\n",
    "        **inputs, generation_config=generation_config, do_sample=True, temperature=temp\n",
    "    )\n",
    "    # print(outputs)\n",
    "    print(f\"\\n° Generated text (temperature = {temp}):\")\n",
    "    print(f\"{tokenizer.decode(outputs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d5bda",
   "metadata": {},
   "source": [
    "Testo generato molto vario al crescere del parametro temperature. Per piccoli valori di temperature si ha il fenomeno della ripetizione del testo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "<span style=\"color: green;\">\n",
    "\n",
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, _at least_ one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "- Since GPT2 is a _autoregressive_ model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select _one_ to use).\n",
    "\n",
    "- BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "- The first _two_ exercises below can probably be done _without_ any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "# Exercise 3.1: Training a Text Classifier (easy) <span style=\"color: red;\">(DONE)</span>\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a _moderately_ sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM _exclusively_ as a feature extractor and then train a shallow model.\n",
    "\n",
    "# Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a _moderately_ sized one and train a model to answer contextualized multiple-choice questions. You _might_ be able to avoid fine-tuning by training a simple model to _rank_ the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure _similarity_ between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the _retrieval_ problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc5b0d",
   "metadata": {},
   "source": [
    "model selected: GPT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27a1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model_pretrained = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9725fc",
   "metadata": {},
   "source": [
    "Dataset scelto: tweet_eval (link: https://huggingface.co/datasets/tweet_eval)\n",
    "\n",
    "TweetEval consists of seven heterogenous tasks in Twitter, all framed as multi-class tweet classification. The tasks include - irony, hate, offensive, stance, emoji, emotion, and sentiment. All tasks have been unified into the same benchmark, with each dataset presented in the same format and with fixed training, validation and test splits.\n",
    "The text in the dataset is in English, as spoken by Twitter users.\n",
    "\n",
    "selected task: emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4d600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5b497b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Sunday afternoon walking through Venice in the sun with @user ️ ️ ️ @ Abbot Kinney, Venice',\n",
       "  \"Time for some BBQ and whiskey libations. Chomp, belch, chomp! (@ Lucille's Smokehouse Bar-B-Que)\",\n",
       "  'Love love love all these people ️ ️ ️ #friends #bff #celebrate #blessed #sundayfunday @ San…',\n",
       "  '️ ️ ️ ️ @ Toys\"R\"Us',\n",
       "  'Man these are the funniest kids ever!! That face! #HappyBirthdayBubb @ FLIPnOUT Xtreme'],\n",
       " 'label': [12, 19, 0, 0, 2]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some example\n",
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b60e3f",
   "metadata": {},
   "source": [
    "### Data Fields\n",
    "\n",
    "For emoji config:\n",
    "\n",
    "- text: a string feature containing the tweet\n",
    "- label: an int classification label with the following mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8f5fe",
   "metadata": {},
   "source": [
    "![labels](./img/labels.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5314fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"Red Heart\",\n",
    "    1: \"Smiling Face with Heart-Eyes\",\n",
    "    2: \"Face with Tears of Joy\",\n",
    "    3: \"Two Hearts\",\n",
    "    4: \"Fire\",\n",
    "    5: \"Smiling Face with Similing Eyes\",\n",
    "    6: \"Similing Face with Sunglasses\",\n",
    "    7: \"Sparkles\",\n",
    "    8: \"Blue Heart\",\n",
    "    9: \"Face Blowing a Kiss\",\n",
    "    10: \"Camera\",\n",
    "    11: \"Flag United States\",\n",
    "    12: \"Sun\",\n",
    "    13: \"Purple Heart\",\n",
    "    14: \"Winking Face\",\n",
    "    15: \"Hundred Points\",\n",
    "    16: \"Beaming Face with Smiling Eyes\",\n",
    "    17: \"Christmas Tree\",\n",
    "    18: \"Camera with Flash\",\n",
    "    19: \"Winking Face with Tongue\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7cca58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(name_model_pretrained)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f42595",
   "metadata": {},
   "source": [
    "Si può notare che a partire dal modello preaddestrato con la classe AutoTokenizer si può ricavare il Tokenizer usato per l'addestramento di tale modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf10444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione per calcolare l'accuracy (viene usata la funzione di sk-learn)\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# funzione per effettuare la tokenizzazione di tutto il dataset (di tutti gli split)\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "tokenized_tweet_eval = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ba2af",
   "metadata": {},
   "source": [
    "Con le istruzioni seguenti settiamo il mostro modello pre-addestrato GPT2\n",
    "Viene impostato l'ID del token di padding (pad_token_id) del modello uguale all'ID del token di fine frase (eos_token_id) del modello.\n",
    "\n",
    "Il token di padding viene utilizzato nelle operazioni di elaborazione del linguaggio naturale, in particolare durante il padding delle sequenze di testo di lunghezza variabile in modo che abbiano tutte la stessa lunghezza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e239a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gpt2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    name_model_pretrained, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "model.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36d7d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./text_classification_model/\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    optim=\"adamw_torch\"\n",
    "    # no_cuda=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_tweet_eval[\"train\"],\n",
    "    eval_dataset=tokenized_tweet_eval[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca30bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/dl23vitcas/anaconda3/envs/Lab2_DLA/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4221' max='4221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4221/4221 31:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.118800</td>\n",
       "      <td>2.575381</td>\n",
       "      <td>0.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.871900</td>\n",
       "      <td>2.448047</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.785200</td>\n",
       "      <td>2.435857</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl23vitcas/anaconda3/envs/Lab2_DLA/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/dl23vitcas/anaconda3/envs/Lab2_DLA/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4221, training_loss=1.9634791276828285, metrics={'train_runtime': 1905.5263, 'train_samples_per_second': 70.847, 'train_steps_per_second': 2.215, 'total_flos': 8070567475322880.0, 'train_loss': 1.9634791276828285, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "223a3045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee22bf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce8db943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl23vitcas/anaconda3/envs/Lab2_DLA/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.8104512691497803,\n",
       " 'eval_accuracy': 0.44628,\n",
       " 'eval_runtime': 165.2223,\n",
       " 'eval_samples_per_second': 302.623,\n",
       " 'eval_steps_per_second': 9.46,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_tweet_eval[\"test\"])  # on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1d951",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abf734cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sequence_to_classify):\n",
    "    # path_models = \"./text_classification_model/checkpoint-4221/\"\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(path_models)\n",
    "    model = trainer.model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name_model_pretrained)\n",
    "    encoded_sequence = tokenizer(sequence_to_classify, return_tensors=\"pt\")\n",
    "    encoded_sequence = encoded_sequence.to(device)\n",
    "    logits = model(**encoded_sequence).logits\n",
    "    predicted_class = logits.argmax().item()\n",
    "    print(\n",
    "        f\"Classe predetta: {predicted_class} ({id2label[predicted_class]}) - score = {torch.softmax(logits, dim=1).squeeze(0)[predicted_class]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6338827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe predetta: 0 (Red Heart) - score = 0.24434871971607208\n",
      "Classe predetta: 0 (Red Heart) - score = 0.2493310272693634\n",
      "Classe predetta: 5 (Smiling Face with Similing Eyes) - score = 0.3650968074798584\n",
      "Classe predetta: 17 (Christmas Tree) - score = 0.48969411849975586\n",
      "Classe predetta: 2 (Face with Tears of Joy) - score = 0.22530613839626312\n",
      "Classe predetta: 7 (Sparkles) - score = 0.202946737408638\n",
      "Classe predetta: 2 (Face with Tears of Joy) - score = 0.5167744159698486\n",
      "Classe predetta: 11 (Flag United States) - score = 0.6926225423812866\n",
      "Classe predetta: 2 (Face with Tears of Joy) - score = 0.28364524245262146\n",
      "Classe predetta: 7 (Sparkles) - score = 0.17856758832931519\n",
      "Classe predetta: 0 (Red Heart) - score = 0.3360006511211395\n"
     ]
    }
   ],
   "source": [
    "inference(\"i love you\")\n",
    "inference(\"i missed you\")\n",
    "inference(\"Happy new Year!\")\n",
    "inference(\"Merry Christmas!\")\n",
    "inference(\"Do you take a photo together?\")\n",
    "inference(\"I'd rather burn myself\")\n",
    "inference(\"fuck you\")\n",
    "inference(\"make america great again\")\n",
    "inference(\"max points\")\n",
    "inference(\"highest quality\")\n",
    "inference(\"best quality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
