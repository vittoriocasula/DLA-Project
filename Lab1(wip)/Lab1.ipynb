{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1faedc",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "Questo laboratorio è stato implementato usando il framework PyTorch. Ho sfruttato questo lavoro per fare pratica con il framework, provare Weights & Bias e soprattutto per cercare di scrivere codice python per fare degli esperimenti che siano indipendenti da vari fattori. L'obiettivo iniziale era quello di costruire delle architetture che siano indipendenti dal dataset usato ma vederemo che i modelli hanno dei vincoli sulla loro struttura che in certi casi vanno in conflitto con la possibilità di astrazione dello stesso.\n",
    "Ho sfruttato wandb per dare la possibilità all'utente utilizzatore del codice di poter interagire con le metriche di performance risultanti dopo l'addestramento (soprattutto semplici grafici, matrici di confusione, ecc..)\n",
    "\n",
    "Nel notebook ho lasciato anche delle porzioni di codice che \"non funzionano\" e di cui si può discutere in fase di esame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a41dba",
   "metadata": {},
   "source": [
    "> ### Installation\n",
    "\n",
    "To get you started, you should be able to recreate an Anaconda environment suitable for this lab using:\n",
    "\n",
    "- conda create -n Lab1_DLA -c conda-forge jupyterlab ipython matplotlib scikit-learn tqdm pytorch torchvision tensorboard\n",
    "\n",
    "- jupyter lab Lab1-CNNs.ipynb\n",
    "\n",
    "> > aggiungi tutti i pacchetti necessari guardando gli import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\">\n",
    "\n",
    "## Exercise 1: Warming Up\n",
    "\n",
    "In this series of exercises I want you to try to duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "We will do this in steps using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "Recall that the main message of the ResNet paper is that **deeper** networks do not **guarantee** more reduction in training loss (or in validation accuracy). Below you will incrementally build a sequence of experiments to verify this for an MLP.\n",
    "\n",
    "### Exercise 1.1: A baseline MLP <span style=\"color: red;\">(DONE)</span>\n",
    "\n",
    "Implement a _simple_ Multilayer Perceptron to classify the 10 digits of MNIST (e.g. two _narrow_ layers). Use my code above as inspiration, but implement your own training pipeline -- you will need it later. Train this model to convergence, monitoring (at least) the loss and accuracy on the training and validation sets for every epoch. Below I include a basic implementation to get you started -- remember that you should write your _own_ pipeline!\n",
    "\n",
    "**Note**: This would be a good time to think about _abstracting_ your model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models.\n",
    "\n",
    "### Exercise 1.2: Rinse and Repeat <span style=\"color: red;\">(DONE)</span>\n",
    "\n",
    "Repeat the verification you did above, but with **Convolutional** Neural Networks. If you were careful about abstracting your model and training code, this should be a simple exercise. Show that **deeper** CNNs _without_ residual connections do not always work better and **even deeper** ones _with_ residual connections.\n",
    "\n",
    "**Hint**: You probably should do this exercise using CIFAR10, since MNIST is _very_ easy (at least up to about 99% accuracy).\n",
    "\n",
    "**Spoiler**: If you plan to do optional exercise 3.3, you should think _very_ carefully about the architectures of your CNNs here (so you can reuse them!).\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Below are **three** exercises that ask you to deepen your understanding of Deep Networks for visual recognition. You must choose **at least one** of the below for your final submission -- feel free to do **more**, but at least **ONE** you must submit.\n",
    "\n",
    "### Exercise 2.1: Explain why Residual Connections are so effective\n",
    "\n",
    "Use your two models (with and without residual connections) you developed above to study and **quantify** why the residual versions of the networks learn more effectively.\n",
    "\n",
    "**Hint**: A good starting point might be looking at the gradient magnitudes passing through the networks during backpropagation.\n",
    "\n",
    "### Exercise 2.2: Fully-convolutionalize a network.\n",
    "\n",
    "Take one of your trained classifiers and **fully-convolutionalize** it. That is, turn it into a network that can predict classification outputs at _all_ pixels in an input image. Can you turn this into a **detector** of handwritten digits? Give it a try.\n",
    "\n",
    "**Hint 1**: Sometimes the process of fully-convolutionalization is called \"network surgery\".\n",
    "\n",
    "**Hint 2**: To test your fully-convolutionalized networks you might want to write some functions to take random MNIST samples and embed them into a larger image (i.e. in a regular grid or at random positions).\n",
    "\n",
    "### Exercise 2.3: _Explain_ the predictions of a CNN <span style=\"color: red;\">(DONE)</span>\n",
    "\n",
    "Use the CNN model you trained in Exercise 1.2 and implement [_Class Activation Maps_](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n",
    "\n",
    "> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "\n",
    "Use your implementation to demonstrate how your trained CNN _attends_ to specific image features to recognize _specific_ classes.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549caeca",
   "metadata": {},
   "source": [
    "> ### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a8282-2322-4dca-b76e-2f3863bc75fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torchviz import make_dot\n",
    "from torchview import draw_graph\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import copy\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    i = 1\n",
    "    device = torch.device(\"cuda:\" + str(i))  # cuda:0 , cuda:1\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # yes, i have a MacBookPro with M1 Pro\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "print(\n",
    "    \"GPU: \" + torch.cuda.get_device_name(i)\n",
    "    if device == torch.device(\"cuda:\" + str(i))\n",
    "    else \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Dataset, Architetture e Ottimizzatore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"cifar10\": {\n",
    "        \"name\": \"cifar10\",\n",
    "        \"initial_example_shape\": (3, 32, 32),\n",
    "        \"num_classes\": 10,\n",
    "    },\n",
    "    \"mnist\": {\n",
    "        \"name\": \"mnist\",\n",
    "        \"initial_example_shape\": (1, 28, 28),\n",
    "        \"num_classes\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "architectures = {\n",
    "    \"MLP\": {\"name\": \"MLP\"},\n",
    "    \"CNN\": {\"name\": \"CNN\"},\n",
    "    \"VGG\": {\"name\": \"VGG\"},\n",
    "    \"ResNet\": {\"name\": \"ResNet\"},\n",
    "    \"PlainNetwork\": {\"name\": \"PlainNetwork\"},\n",
    "    \"CNN_GAP\": {\"name\": \"CNN_GAP\"},\n",
    "}\n",
    "\n",
    "optimizers = {\"sgd\": {\"name\": \"sgd\"}, \"adam\": {\"name\": \"adam\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt of how to choose an optimal initial learning rate (doesn't work).\n",
    "# I try to replicate an idea of karpathy's youtube video\n",
    "# link: https://www.youtube.com/watch?v=TCH_1BHY58I&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3\n",
    "# around 45 minute\n",
    "\"\"\"\n",
    "dataset = MNIST(\n",
    "    root=\"../dataset/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "# steps = len(dataset)\n",
    "steps = 1000\n",
    "lre = torch.linspace(-3, 0, steps)\n",
    "lrs = 10**lre\n",
    "model = MLP(input_size=1 * 28 * 28, width=10, depth=20, output_size=10).to(\n",
    "    device\n",
    ")  # input_size, width, depth, output_size\n",
    "\n",
    "lri = []\n",
    "lossi = []\n",
    "for i in range(steps):\n",
    "    xs, ys = dataset[i]\n",
    "    xs = xs.float().to(device)\n",
    "    ys = torch.tensor([ys]).to(device)\n",
    "    logits = model(xs)\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = lrs[i]\n",
    "    for p in model.parameters():\n",
    "        p.data += -lr * p.grad\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())\n",
    "plt.plot(lri, lossi)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Setting Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data[\"cifar10\"]\n",
    "architecture = architectures[\"MLP\"]\n",
    "optmizer = optimizers[\"adam\"]\n",
    "\n",
    "batch_size = 64\n",
    "val_size = 10000\n",
    "epochs = 10\n",
    "lr = 0.1\n",
    "resize_values = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(\n",
    "    dataset[\"initial_example_shape\"]\n",
    ")  # la shape è una tuple, per modificarla occorre passare da lista\n",
    "lst[1] = resize_values[0]  # modifica le dimensioni spaziali delle immagini\n",
    "lst[2] = resize_values[1]  # modifica le dimensioni spaziali delle immagini\n",
    "example_shape_resized = tuple(lst)\n",
    "\n",
    "input_size = result = reduce(lambda a, b: a * b, example_shape_resized)  # i.e. 3*32*32\n",
    "batch_shape = (batch_size,) + example_shape_resized  # ex. (64,1,28,28)\n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"batch_shape: {batch_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Setting WANDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Lab1.ipynb\"\n",
    "\n",
    "wandb.init(\n",
    "    job_type=f'{dataset[\"name\"]}',\n",
    "    name=f'{architecture[\"name\"]}',  # name of run\n",
    "    project=\"Lab1_DLA\",\n",
    "    config={\n",
    "        \"dataset\": dataset[\"name\"],\n",
    "        \"architecture\": architecture[\"name\"],\n",
    "        \"initial_lr\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch size\": batch_size,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc12cc-8422-47bf-8d8e-0950ac05ae96",
   "metadata": {},
   "source": [
    "> ### Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot MNIST dataset distribution\n",
    "def plot_hist_labels_distribution(data, num_classes, split, position):\n",
    "    if split == \"train\":\n",
    "        all_labels = torch.tensor(\n",
    "            [tensor.item() for tensor in list(Counter(data.dataset.targets).keys())]\n",
    "        ).float()\n",
    "    elif split == \"val\":\n",
    "        labels_val = [y for _, y in data]\n",
    "        all_labels = torch.tensor([value for value in list(labels_val)]).float()\n",
    "    elif split == \"test\":\n",
    "        all_labels = torch.tensor(\n",
    "            [tensor.item() for tensor in list(Counter(data.targets).keys())]\n",
    "        ).float()\n",
    "\n",
    "    hist = torch.histc(all_labels, bins=num_classes)\n",
    "    plt.subplot(1, 3, position)\n",
    "    plt.bar(range(num_classes), hist, align=\"center\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Number of Examples\")\n",
    "    plt.xticks(np.arange(0, num_classes, 1))\n",
    "    plt.title(split + \" \" + \"Set Label Distribution\")\n",
    "\n",
    "\n",
    "def plot_hist_labels_distribution_string(data, num_classes, split, position):\n",
    "    if split == \"train\":\n",
    "        labels_train = [y for _, y in data]\n",
    "        all_labels = torch.tensor([value for value in list(labels_train)]).float()\n",
    "        classes = data.dataset.classes\n",
    "    elif split == \"val\":\n",
    "        labels_val = [y for _, y in data]\n",
    "        all_labels = torch.tensor([value for value in list(labels_val)]).float()\n",
    "        classes = data.dataset.classes\n",
    "    elif split == \"test\":\n",
    "        all_labels = torch.tensor([value for value in list(data.targets)]).float()\n",
    "        classes = data.classes\n",
    "\n",
    "    hist = torch.histc(all_labels, bins=num_classes)\n",
    "    plt.subplot(1, 3, position)\n",
    "    plt.bar(range(num_classes), hist, align=\"center\")\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Number of Examples\")\n",
    "    plt.xticks(np.arange(0, num_classes, 1), classes, rotation=90)\n",
    "    plt.title(split + \" \" + \"Set Label Distribution\")\n",
    "\n",
    "\n",
    "# for mnist: get_normalize_values(ds_train_initial.data.float())\n",
    "# for cifar10: get_normalize_values(ds_train_initial.data)\n",
    "\n",
    "\n",
    "def get_normalize_values(data):\n",
    "    return (\n",
    "        data.mean(axis=(0, 1, 2)) / 255,\n",
    "        data.std(axis=(0, 1, 2)) / 255,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a69db-0416-444a-9be4-5f055ff48bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "match dataset[\"name\"]:\n",
    "    case \"mnist\":\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                transforms.Resize(size=resize_values, antialias=True),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        ds_train_initial = MNIST(\n",
    "            root=\"../dataset/\", train=True, download=True, transform=transform\n",
    "        )\n",
    "        ds_test = MNIST(\n",
    "            root=\"../dataset/\", train=False, download=True, transform=transform\n",
    "        )\n",
    "\n",
    "        n_examples_test = len(ds_test)\n",
    "        I = np.random.permutation(len(ds_train_initial))\n",
    "        ds_val = Subset(ds_train_initial, I[:val_size])\n",
    "        n_examples_val = len(ds_val)\n",
    "\n",
    "        ds_train = Subset(ds_train_initial, I[val_size:])\n",
    "        n_examples_train = len(ds_train)\n",
    "\n",
    "        print(f\"N° samples train: {n_examples_train}\")\n",
    "        print(f\"N° samples validation: {n_examples_val}\")\n",
    "        print(f\"N° samples test: {n_examples_test}\")\n",
    "        # Imposta la dimensione della figura per contenere i 3 grafici in riga\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plot_hist_labels_distribution(ds_train, dataset[\"num_classes\"], \"train\", 1)\n",
    "        plot_hist_labels_distribution(ds_val, dataset[\"num_classes\"], \"val\", 2)\n",
    "        plot_hist_labels_distribution(ds_test, dataset[\"num_classes\"], \"test\", 3)\n",
    "        plt.tight_layout()  # Ottimizza lo spaziamento tra i grafici\n",
    "\n",
    "    case \"cifar10\":\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    (0.49139968, 0.48215841, 0.44653091),\n",
    "                    (0.24703223, 0.24348513, 0.26158784),\n",
    "                ),\n",
    "                transforms.Resize(size=resize_values, antialias=True),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        ds_train_initial = CIFAR10(\n",
    "            root=\"../dataset/\", train=True, download=False, transform=transform\n",
    "        )\n",
    "\n",
    "        ds_test = CIFAR10(\n",
    "            root=\"../dataset/\", train=False, download=False, transform=transform\n",
    "        )\n",
    "\n",
    "        n_examples_test = len(ds_test)\n",
    "\n",
    "        print(f\"Classes: {ds_train_initial.class_to_idx}\")\n",
    "\n",
    "        I = np.random.permutation(len(ds_train_initial))\n",
    "        ds_val = Subset(ds_train_initial, I[:val_size])\n",
    "        n_examples_val = len(ds_val)\n",
    "\n",
    "        ds_train = Subset(ds_train_initial, I[val_size:])\n",
    "        n_examples_train = len(ds_train)\n",
    "\n",
    "        print(f\"N° samples train: {n_examples_train}\")\n",
    "        print(f\"N° samples validation: {n_examples_val}\")\n",
    "        print(f\"N° samples test: {n_examples_test}\")\n",
    "        # Imposta la dimensione della figura per contenere i 3 grafici in riga\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plot_hist_labels_distribution_string(\n",
    "            ds_train, dataset[\"num_classes\"], \"train\", 1\n",
    "        )\n",
    "        plot_hist_labels_distribution_string(ds_val, dataset[\"num_classes\"], \"val\", 2)\n",
    "        plot_hist_labels_distribution_string(ds_test, dataset[\"num_classes\"], \"test\", 3)\n",
    "\n",
    "        plt.tight_layout()  # Ottimizza lo spaziamento tra i grafici\n",
    "\n",
    "plt.savefig(\"img/dataset_label_distribution.png\")\n",
    "wandb.log(\n",
    "    {\"Dataset Label Distribution\": wandb.Image(\"img/dataset_label_distribution.png\")}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_array(dataloader):\n",
    "    first_batch = (next(iter(dataloader))[0]).numpy()\n",
    "    list_batch = np.split(first_batch, len(first_batch))\n",
    "\n",
    "    if list_batch[0].shape[1] == 1:  # single channels images\n",
    "        image_array = np.array([el.squeeze() for el in list_batch])\n",
    "        images = [\n",
    "            PIL.Image.fromarray((image * 255).astype(np.uint8)) for image in image_array\n",
    "        ]\n",
    "    else:  # multiple channels images\n",
    "        image_array = np.array([el.squeeze().transpose(1, 2, 0) for el in list_batch])\n",
    "        images = [\n",
    "            PIL.Image.fromarray((image * 255).astype(\"uint8\"), \"RGB\")\n",
    "            for image in image_array\n",
    "        ]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size, shuffle=True, num_workers=4\n",
    ")\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size, num_workers=4)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "num_batch_train = len(dl_train)\n",
    "num_batch_val = len(dl_val)\n",
    "num_batch_test = len(dl_test)\n",
    "\n",
    "print(f\"N° batches train: {num_batch_train}\")\n",
    "print(f\"N° batches val: {num_batch_val}\")\n",
    "print(f\"N° batches test: {num_batch_test}\")\n",
    "\n",
    "# visualize in wandb a batch of images\n",
    "wandb.log(\n",
    "    {\n",
    "        \"examples train\": [wandb.Image(image) for image in get_image_array(dl_train)],\n",
    "        \"examples val\": [wandb.Image(image) for image in get_image_array(dl_val)],\n",
    "        \"examples test\": [wandb.Image(image) for image in get_image_array(dl_test)],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e05e96-7707-4490-98b8-50cb5e330af1",
   "metadata": {},
   "source": [
    "> ### Training and evaluation Epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcce348-f603-4d57-b9a8-5b1c6eba28ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to train a model for a single epoch over the data loader.\n",
    "def train_epoch(model, dl, opt, epoch, device):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for xs, ys in tqdm(dl, desc=f\"Training epoch {epoch}\", leave=True, position=0):\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(xs)\n",
    "        loss = F.cross_entropy(logits, ys)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        # accumulate values\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, dl, device, split_name=\"\"):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs, ys in tqdm(\n",
    "            dl, desc=\"\\tEvaluating on \" + split_name, leave=True, position=0\n",
    "        ):\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "\n",
    "            logits = model(xs)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            loss = F.cross_entropy(logits, ys)\n",
    "\n",
    "            # accumulate values\n",
    "            losses.append(loss.item())\n",
    "            gts.append(ys.detach().cpu().numpy())\n",
    "            predictions.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    return gts, predictions, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875008c3-306c-4e39-a845-d7bda7862621",
   "metadata": {},
   "source": [
    "> ## Architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b07d0",
   "metadata": {},
   "source": [
    "> ## MLP\n",
    "\n",
    "- input_size: dimensione delle feature in ingresso. Nel caso multidimensionale viene eseguito un flatten\n",
    "- width: numero di neuroni degli strati linear (vincolo: tutti i layer hanno la stessa width)\n",
    "- depth: numero di hidden layer\n",
    "- output_size: numero di neuroni nell'ultimo layer linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e503a-37df-4fb9-94e7-85d0adb494bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, width, depth, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, width)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(width, width) for _ in range(depth)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(width, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.input_layer(x)\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        # insert ReLU between each Linear layer\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "            out = torch.relu(out)\n",
    "\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be292d2",
   "metadata": {},
   "source": [
    "> ## CNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1a7d4",
   "metadata": {},
   "source": [
    "- input_channels: numero di canali dell'esempio in ingresso (nel caso di immagini questo è uguale a 3)\n",
    "- output_size: numero di neuroni nell'ultimo layer linear\n",
    "- depth: numero di hidden layer\n",
    "- channels_intermediate: numero di canali degli strati convoluzionali intermedi alla rete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_size, depth, channels_intermediate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.depth = depth\n",
    "        self.channels_intermediate = channels_intermediate\n",
    "\n",
    "        self.kernel_size = (5, 5)\n",
    "        self.kernel_size_pool = (2, 2)\n",
    "\n",
    "        self.first_conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_channels,\n",
    "                out_channels=self.channels_intermediate,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=self.kernel_size_pool),\n",
    "        )\n",
    "\n",
    "        self.intermediate_block = nn.ModuleList()\n",
    "        for _ in range(self.depth):\n",
    "            self.intermediate_block.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=self.channels_intermediate,\n",
    "                    out_channels=self.channels_intermediate,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                )\n",
    "            )\n",
    "            self.intermediate_block.append(nn.ReLU())\n",
    "            self.intermediate_block.append(\n",
    "                nn.MaxPool2d(kernel_size=self.kernel_size_pool)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.first_conv(x)\n",
    "\n",
    "        for layer in self.intermediate_block:\n",
    "            out = layer(out)\n",
    "\n",
    "        self.input_fcs = reduce(lambda a, b: a * b, out.shape[1:])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_fcs, 300).to(device)\n",
    "        self.fc2 = nn.Linear(300, 100).to(device)\n",
    "        self.fc3 = nn.Linear(100, self.output_size).to(device)\n",
    "\n",
    "        out = out.flatten(1)\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.randn(64,3,32,32).to(device)\n",
    "m = CNN(input_channels= x.shape[1], output_size= 10, depth=2, channels_intermediate=20).to(device)\n",
    "res = m(x)\n",
    "print(res.shape)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457133ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_GAP(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(CNN_GAP, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # Adaptive AvgPooling to any input size\n",
    "        self.fc = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def compute_cam(self, x, class_label):\n",
    "        x = x.to(device)\n",
    "        self.eval()\n",
    "        # class_weight: sono i pesi che sono collegati al neurone dell'ultimo layer corrispondente alla classe scelta\n",
    "        # class_weight.shape ex. torch.Size([10, 128])\n",
    "        class_weight = (list(self.parameters())[-2]).data\n",
    "        # channels: numero di canali dell'ultimo layer convoluzionale (canali della feature map)\n",
    "        channels = class_weight.shape[1]\n",
    "        layers = list(self.modules())\n",
    "        index_avg_poll = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            if isinstance(layer, nn.AvgPool2d) or isinstance(\n",
    "                layer, nn.AdaptiveAvgPool2d\n",
    "            ):\n",
    "                index_avg_poll = i\n",
    "        with torch.no_grad():\n",
    "            detectors = list(self.modules())[1 : -(len(layers) - index_avg_poll)]\n",
    "            # act_map.shape ex. torch.Size([64, 128, 4, 4]) (N.B. non è compreso l'avg pooling)\n",
    "            act_map = reduce(lambda x, f: f(x), detectors, x)\n",
    "        # weights: pesi relativi al neurone class_label (ex. torch.Size([128]) )\n",
    "        weights = class_weight[class_label]\n",
    "        weights = weights.view(-1, channels, 1, 1)  # (1, 128, 1, 1)\n",
    "        cam = act_map * weights  # (64, 128, 4, 4) * (1, 128, 1, 1) = (64, 128, 4, 4)\n",
    "        # cam = F.relu(cam.sum(dim=1, keepdim=True))  # (64, 1, 4, 4) #TODO insert relu or no?\n",
    "        cam = cam.sum(dim=1, keepdim=True)  # (64, 1, 4, 4)\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bcf844",
   "metadata": {},
   "source": [
    "La figura mostra i modelli che sono descritti nel paper \"Deep Residual Learning for Image Recognition\". Nei prossimi blocchi di codice ho implementato \"from scratch\" tali architetture. I dataset definiti in precedenza sono composti da immagini le cui dimensioni sono piuttosto piccole per architetture troppo profonde. Ho infatti deciso di effettuare un \"Resize\" di queste immagini ad un dimensione più alta (circa 4 volte in più) per risolvere tale problema\n",
    "\n",
    "![nets](./img/nets.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24a8af",
   "metadata": {},
   "source": [
    "Si può notare che la prima rete VGG termina con un maxpooling e 3 strati dense mentre le altre due architetture hanno un Gloabal Avarage Pooling e un solo strato dense. La prima architettura non permette quindi di calcolare la Class Activation Map (CAM) per costruzione. Le altre due invece hanno una struttura idonea all'applicazione della CAM proprio come descrive il paper ufficiale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52491c",
   "metadata": {},
   "source": [
    "> ## VGG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_types = {\n",
    "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"VGG13\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "    \"VGG16\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "    \"VGG19\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e84428",
   "metadata": {},
   "source": [
    "E' possibile instanziale le tipiche architetture VGG (mediante l'argomento di input architecture): VGG11, VGG13, VGG16 e VGG19.\n",
    "Nella variabile VGG_types è possibile vedere l'alternarsi di valori interi e stringhe. I valori interi indicano i canali degli strati convoluzionali della rete (in ordine con cui vengono applicati all'input). Le stringhe (presente solo \"M\") indicano la presenza di un MaxPool2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        architecture,\n",
    "        in_channels=3,\n",
    "        in_height=224,\n",
    "        in_width=224,\n",
    "        num_hidden=4096,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super(VGG, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.in_width = in_width\n",
    "        self.in_height = in_height\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_classes = num_classes\n",
    "        self.convs = self.init_convs(architecture)\n",
    "        self.fcs = self.init_fcs(architecture)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    def init_convs(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == int:\n",
    "                out_channels = x\n",
    "                layers.extend(\n",
    "                    [\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=out_channels,\n",
    "                            kernel_size=(3, 3),\n",
    "                            stride=(1, 1),\n",
    "                            padding=(1, 1),\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU(),\n",
    "                    ]\n",
    "                )\n",
    "                in_channels = x\n",
    "            else:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def init_fcs(self, architecture):\n",
    "        pool_count = architecture.count(\"M\")\n",
    "\n",
    "        factor = 2**pool_count\n",
    "\n",
    "        if (self.in_height % factor) + (self.in_width % factor) != 0:\n",
    "            raise ValueError(\n",
    "                f\"`in_height` and `in_width` must be multiples of {factor}\"\n",
    "            )\n",
    "        out_height = self.in_height // factor\n",
    "        out_width = self.in_width // factor\n",
    "\n",
    "        last_out_channels = next(x for x in architecture[::-1] if type(x) == int)\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(last_out_channels * out_height * out_width, self.num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.num_hidden, self.num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.num_hidden, self.num_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.randn(64, 3, 32, 32).to(device)\n",
    "m = VGG(\n",
    "    in_channels=3,\n",
    "    in_height=32,\n",
    "    in_width=32,\n",
    "    num_hidden=512,\n",
    "    architecture=VGG_types[\"VGG11\"],\n",
    ").to(device)\n",
    "res = m(x)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea37911",
   "metadata": {},
   "source": [
    "> ## PlainNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_modules(model):\n",
    "    individual_modules = []\n",
    "\n",
    "    def collect_modules(module):\n",
    "        for layer in module.children():\n",
    "            if isinstance(layer, nn.Sequential) or isinstance(layer, SequentialBlock):\n",
    "                # Se il layer è un modulo Sequential, richiama ricorsivamente la funzione\n",
    "                collect_modules(layer)\n",
    "            else:\n",
    "                # Aggiungi il modulo singolo alla lista\n",
    "                individual_modules.append(layer)\n",
    "\n",
    "    collect_modules(model)\n",
    "\n",
    "    return individual_modules\n",
    "\n",
    "\n",
    "# CNN no residuals\n",
    "class SequentialBlock(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, out_channels, stride=1):\n",
    "        super(SequentialBlock, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=3, stride=stride, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "\n",
    "class PlainNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers, block, image_channels, num_classes=dataset[\"num_classes\"]\n",
    "    ):\n",
    "        super(PlainNetwork, self).__init__()\n",
    "\n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "        elif num_layers == 34 or num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.first_layers = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.layer1 = self.make_layers(\n",
    "            num_layers, block, layers[0], intermediate_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self.make_layers(\n",
    "            num_layers, block, layers[1], intermediate_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self.make_layers(\n",
    "            num_layers, block, layers[2], intermediate_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self.make_layers(\n",
    "            num_layers, block, layers[3], intermediate_channels=512, stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_layers(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def make_layers(\n",
    "        self, num_layers, block, num_residual_blocks, intermediate_channels, stride\n",
    "    ):\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(num_layers, self.in_channels, intermediate_channels, stride)\n",
    "        )\n",
    "        self.in_channels = intermediate_channels\n",
    "        for _ in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def compute_cam(self, x, class_label):\n",
    "        x = x.to(device)\n",
    "        self.eval()\n",
    "        # class_weight.shape --> torch.Size([10, 128])\n",
    "        # sono i pesi che sono collegati al neurone dell'ultimo layer corrispondente alla classe scelta\n",
    "        class_weight = (list(self.parameters())[-2]).data\n",
    "        channels = class_weight.shape[1]\n",
    "        # numero di canali dell'ultimo layer convoluzionale (canali della feature map)\n",
    "        layers = get_individual_modules(self)\n",
    "        index_avg_poll = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            if isinstance(layer, nn.AdaptiveAvgPool2d):\n",
    "                index_avg_poll = i\n",
    "\n",
    "        with torch.no_grad():\n",
    "            detectors = layers[0 : -(len(layers) - index_avg_poll)]\n",
    "            # act_map.shape --> torch.Size([64, 128, 4, 4])\n",
    "            act_map = reduce(lambda x, f: f(x), detectors, x)\n",
    "        # torch.Size([128]) ricavo i pesi relativi al neurone class_label\n",
    "        weights = class_weight[class_label]\n",
    "        weights = weights.view(-1, channels, 1, 1)  # (1, 128, 1, 1)\n",
    "        cam = act_map * weights  # (64, 128, 4, 4) * (1, 128, 1, 1) = (64, 128, 4, 4)\n",
    "        # cam = F.relu(cam.sum(dim=1, keepdim=True))  # (64, 1, 4, 4)\n",
    "        cam = cam.sum(dim=1, keepdim=True)  # (64, 1, 4, 4)\n",
    "        return cam\n",
    "\n",
    "\n",
    "def CNN18Plain(img_channels=batch_shape[1]):\n",
    "    return PlainNetwork(18, SequentialBlock, img_channels)\n",
    "\n",
    "\n",
    "def CNN34Plain(img_channels=batch_shape[1]):\n",
    "    return PlainNetwork(34, SequentialBlock, img_channels)\n",
    "\n",
    "\n",
    "def CNN50Plain(img_channels=batch_shape[1]):\n",
    "    return PlainNetwork(50, SequentialBlock, img_channels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.randn(64, 3, 32, 32).to(device)\n",
    "model = CNN34Plain(3, 10).to(device)\n",
    "res = model(x)\n",
    "print(res.shape)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "class_target = 3\n",
    "x, y = next(iter(dl_train))\n",
    "model = CNN34Plain(batch_shape[1], dataset[\"num_classes\"]).to(device)\n",
    "model.load_state_dict(torch.load(path_models))\n",
    "print(*get_individual_modules(model), sep=\"\\n\")\n",
    "cam = model.compute_cam(x, class_target)\n",
    "\"\"\"\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb8a482",
   "metadata": {},
   "source": [
    "> ## ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockResidual(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1\n",
    "    ):\n",
    "        super(BlockResidual, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers, block, image_channels, num_classes=dataset[\"num_classes\"]\n",
    "    ):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "        elif num_layers == 34 or num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNetLayers\n",
    "        self.layer1 = self.make_layers(\n",
    "            num_layers, block, layers[0], intermediate_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self.make_layers(\n",
    "            num_layers, block, layers[1], intermediate_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self.make_layers(\n",
    "            num_layers, block, layers[2], intermediate_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self.make_layers(\n",
    "            num_layers, block, layers[3], intermediate_channels=512, stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def make_layers(\n",
    "        self, num_layers, block, num_residual_blocks, intermediate_channels, stride\n",
    "    ):\n",
    "        layers = []\n",
    "        identity_downsample = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                self.in_channels, intermediate_channels, kernel_size=1, stride=stride\n",
    "            ),\n",
    "            nn.BatchNorm2d(intermediate_channels),\n",
    "        )\n",
    "\n",
    "        layers.append(\n",
    "            block(\n",
    "                num_layers,\n",
    "                self.in_channels,\n",
    "                intermediate_channels,\n",
    "                identity_downsample,\n",
    "                stride,\n",
    "            )\n",
    "        )\n",
    "        self.in_channels = intermediate_channels\n",
    "        for _ in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def compute_cam(self, x, class_label):\n",
    "        x = x.to(device)\n",
    "        self.eval()\n",
    "        class_weight = (list(self.parameters())[-2]).data\n",
    "\n",
    "        # class_weight.shape --> torch.Size([10, 128]) # sono i pesi che sono collegati al neurone dell'ultimo layer corrispondente alla classe scelta\n",
    "        channels = class_weight.shape[\n",
    "            1\n",
    "        ]  # numero di canali dell'ultimo layer convoluzionale (canali della feature map)\n",
    "\n",
    "        layers = get_individual_modules(self)\n",
    "\n",
    "        index_avg_poll = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            if isinstance(layer, nn.AdaptiveAvgPool2d):\n",
    "                index_avg_poll = i\n",
    "\n",
    "        with torch.no_grad():\n",
    "            detectors = layers[0 : -(len(layers) - index_avg_poll)]\n",
    "            print(*detectors, sep=\"\\n\")\n",
    "\n",
    "            act_map = reduce(\n",
    "                lambda x, f: f(x), detectors, x\n",
    "            )  # act_map.shape --> torch.Size([64, 128, 4, 4])\n",
    "\n",
    "        weights = class_weight[\n",
    "            class_label\n",
    "        ]  # torch.Size([128]) ricavo i pesi relativi al neurone class_label\n",
    "        weights = weights.view(-1, channels, 1, 1)  # (1, 128, 1, 1)\n",
    "        cam = act_map * weights  # (64, 128, 4, 4) * (1, 128, 1, 1) = (64, 128, 4, 4)\n",
    "        cam = cam.sum(dim=1, keepdim=True)  # (64, 1, 4, 4)\n",
    "\n",
    "        # cam = F.relu(cam.sum(dim=1, keepdim=True))  # (64, 1, 4, 4)\n",
    "        return cam\n",
    "\n",
    "\n",
    "def ResNet18(img_channels=batch_shape[1]):\n",
    "    return ResNet(18, BlockResidual, img_channels)\n",
    "\n",
    "\n",
    "def ResNet34(img_channels=batch_shape[1]):\n",
    "    return ResNet(34, BlockResidual, img_channels)\n",
    "\n",
    "\n",
    "def ResNet50(img_channels=batch_shape[1]):\n",
    "    return ResNet(50, BlockResidual, img_channels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.randn(64, 3, 32, 32).to(device)\n",
    "model = ResNet34(3, 10).to(device)\n",
    "res = model(x)\n",
    "print(res.shape)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06e26-8fa3-414e-a502-8d1c18ba9eb7",
   "metadata": {},
   "source": [
    "> ## Creazione del modello\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b57a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "match architecture[\"name\"]:\n",
    "    case \"MLP\":\n",
    "        model = MLP(input_size, width=128, depth=4, output_size=dataset[\"num_classes\"])\n",
    "    case \"CNN\":\n",
    "        model = CNN(\n",
    "            input_channels=batch_shape[1],\n",
    "            output_size=dataset[\"num_classes\"],\n",
    "            depth=1,\n",
    "            channels_intermediate=32,\n",
    "        )\n",
    "    case \"CNN_GAP\":\n",
    "        model = CNN_GAP(\n",
    "            input_channels=batch_shape[1], output_size=dataset[\"num_classes\"]\n",
    "        )\n",
    "    case \"VGG\":\n",
    "        # type_vgg available: \"VGG11\", \"VGG13\", \"VGG16\", \"VGG19\"\n",
    "        type_vgg = \"VGG19\"\n",
    "        model = VGG(\n",
    "            in_channels=batch_shape[1],\n",
    "            in_height=batch_shape[2],\n",
    "            in_width=batch_shape[3],\n",
    "            architecture=VGG_types[type_vgg],\n",
    "        )\n",
    "    case \"PlainNetwork\":\n",
    "        # depth available: \"18\", \"34\", \"50\"\n",
    "        depth = \"18\"\n",
    "        match depth:\n",
    "            case \"18\":\n",
    "                model = CNN18Plain(batch_shape[1])\n",
    "            case \"34\":\n",
    "                model = CNN34Plain(batch_shape[1])\n",
    "            case \"50\":\n",
    "                model = CNN50Plain(batch_shape[1])\n",
    "    case \"ResNet\":\n",
    "        # depth available: \"18\", \"34\", \"50\"\n",
    "        depth = \"34\"\n",
    "        match depth:\n",
    "            case \"18\":\n",
    "                model = ResNet18(batch_shape[1])\n",
    "            case \"34\":\n",
    "                model = ResNet34(batch_shape[1])\n",
    "            case \"50\":\n",
    "                model = ResNet50(batch_shape[1])\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'Architecture name: {architecture[\"name\"]}')\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"N° of parameters: {params}\")\n",
    "print(f\"Model is in {next(model.parameters()).device}\")\n",
    "print(\"\\n\\nLayers of the network sorted:\\n\")\n",
    "print(*get_individual_modules(model), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91360aad",
   "metadata": {},
   "source": [
    "> ## Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe378b",
   "metadata": {},
   "source": [
    "In questa sezione è possibile variare l'ottimizzatore e i suoi iperparametri. Il learning rate iniziale è fisso ma questo varia durante l'addestramento attraverso uno strumento chiamato scheduler.\n",
    "\n",
    "In generale il lr può essere variato in base al numero di epoche oppure alle performance su un validation set. In genere il lr si vuole decrementare e non incrementare ma dipende molto dal problema che stiamo affrontando.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match optmizer[\"name\"]:\n",
    "    case \"adam\":\n",
    "        # betas, eps, weight_decay\n",
    "        opt = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    case \"sgd\":\n",
    "        # momentum, weight_decay\n",
    "        opt = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "\n",
    "sched = lr_scheduler.ReduceLROnPlateau(optimizer=opt, mode=\"min\", patience=5)\n",
    "print(f'Optmizer: {optmizer[\"name\"]} \\n\\n{opt.state_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of pytorch models in the form of visual graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_copy = copy.deepcopy(model)\n",
    "model_graph = draw_graph(model_copy, input_size=batch_shape, device=\"meta\")\n",
    "image = model_graph.visual_graph\n",
    "image.save(\"img/visual_graph\")\n",
    "wandb.log({\"Model Visual Graph\": wandb.Image(image.render(format=\"png\", cleanup=True))})\n",
    "# image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55610f21",
   "metadata": {},
   "source": [
    "## Computational Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b68a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(batch_shape).to(device)\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 123626472\n",
    "\n",
    "out = model(input_tensor)\n",
    "graph = make_dot(\n",
    "    out, params=dict(model.named_parameters()), show_attrs=True, show_saved=True\n",
    ")\n",
    "graph.save(\"img/computational_graph\")\n",
    "wandb.log(\n",
    "    {\"Computational Graph\": wandb.Image(graph.render(format=\"png\", cleanup=True))}\n",
    ")\n",
    "# graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e3f4e",
   "metadata": {},
   "source": [
    "> ## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradients_magnitudes(model, epoch):\n",
    "    # loss.backward REQUIRED\n",
    "    gradient_magnitudes = []\n",
    "    names_layers = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            gradient_magnitudes.append(param.grad.norm().item())\n",
    "            names_layers.append(name)\n",
    "    # Plot dei gradienti\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(len(gradient_magnitudes)), gradient_magnitudes)\n",
    "    plt.xlabel(\"Parameters Layers\")\n",
    "    plt.ylabel(\"Gradient Magnitude\")\n",
    "    plt.title(\"Epoch: \" + str(epoch) + \" - Gradient Magnitudes of Model Parameters\")\n",
    "\n",
    "    dir_path = \"gradient_magnitudes/\" + architecture[\"name\"]\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    plt_path = dir_path + \"/gradient_magnitudes_epoch_\" + str(epoch) + \".png\"\n",
    "\n",
    "    plt.savefig(plt_path)\n",
    "    data = [\n",
    "        [name, magnitude] for name, magnitude in zip(names_layers, gradient_magnitudes)\n",
    "    ]\n",
    "    table = wandb.Table(data=data, columns=[\"Parameter\", \"Gradient Magnitude\"])\n",
    "    wandb.log(\n",
    "        {\n",
    "            f\"Gradient Magnitudes Epoch {epoch}\": wandb.plot.bar(\n",
    "                table,\n",
    "                \"Parameter\",\n",
    "                \"Gradient Magnitude\",\n",
    "                title=\"Gradient Magnitudes of Model Parameters - Epoch \" + str(epoch),\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89e48f-d8f3-4122-842d-1ff389499854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"./models/\", exist_ok=True)\n",
    "path_models = \"./models/model_\" + dataset[\"name\"] + \"_\" + architecture[\"name\"] + \".pth\"\n",
    "\n",
    "if not os.path.exists(path_models):  # se non esiste fai l'addestramento\n",
    "    for epoch in range(epochs):\n",
    "        losses_train = train_epoch(model, dl_train, opt, epoch, device=device)\n",
    "\n",
    "        plot_gradients_magnitudes(model, epoch)\n",
    "\n",
    "        gts, predictions, losses_val = evaluate_model(\n",
    "            model, dl_val, device=device, split_name=\"val\"\n",
    "        )\n",
    "\n",
    "        loss_val = np.mean(losses_val)\n",
    "        sched.step(loss_val)\n",
    "        val_acc = accuracy_score(np.hstack(gts), np.hstack(predictions))\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"accuracy validation\": val_acc,\n",
    "                \"loss training\": np.mean(losses_train),\n",
    "                \"loss validation:\": loss_val,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    torch.save(model.state_dict(), path_models)\n",
    "else:\n",
    "    print(f\"Model exist on path {path_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./gradient_magnitudes/\" + architecture[\"name\"]\n",
    "\n",
    "immagini = os.listdir(path)\n",
    "num_colonne = 3  # Imposta il numero di colonne desiderato\n",
    "num_immagini = len(immagini)\n",
    "num_righe = -(-num_immagini // num_colonne)  # Calcola il numero di righe necessario\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    num_righe, num_colonne, figsize=(num_colonne * 5, num_righe * 5)\n",
    ")\n",
    "\n",
    "for i, img in enumerate(immagini):\n",
    "    riga = i // num_colonne\n",
    "    colonna = i % num_colonne\n",
    "    image_path = os.path.join(path, img)\n",
    "    img = Image.open(image_path)\n",
    "    axes[riga, colonna].imshow(img)\n",
    "    axes[riga, colonna].axis(\"off\")  # Nasconde i valori degli assi\n",
    "\n",
    "# Nasconde gli assi per gli eventuali assi non utilizzati\n",
    "for r in range(num_righe):\n",
    "    for c in range(num_colonne):\n",
    "        if r * num_colonne + c >= num_immagini:\n",
    "            axes[r, c].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()  # Impedisce sovrapposizioni di layout\n",
    "plt.savefig(\"img/gradient_magnitude.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(path_models))\n",
    "gts, predictions, losses_val = evaluate_model(\n",
    "    model, dl_test, device=device, split_name=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = np.mean(losses_val)\n",
    "val_acc = accuracy_score(np.hstack(gts), np.hstack(predictions))\n",
    "report_acc = classification_report(\n",
    "    np.hstack(gts), np.hstack(predictions), zero_division=0, digits=3\n",
    ")\n",
    "print(f\"Accuracy report on TEST:\\n {report_acc}\")\n",
    "print(f\"\\nAccuracy at {epochs} epoch on TEST:\\n {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gts = [item for sublist in gts for item in sublist]\n",
    "all_predictions = [item for sublist in predictions for item in sublist]\n",
    "print(\n",
    "    f\"Confusione Matrix Test Set\\n{confusion_matrix(all_gts, all_predictions, labels=torch.arange(0,10))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    classification_report(\n",
    "        np.hstack(gts),\n",
    "        np.hstack(predictions),\n",
    "        zero_division=0,\n",
    "        digits=3,\n",
    "        output_dict=True,\n",
    "    )\n",
    ").transpose()\n",
    "df_label = df[:10]\n",
    "df_label.insert(0, \"class_label\", ds_test.classes)\n",
    "df_label.loc[:, \"support\"] = df_label[\"support\"].astype(int)\n",
    "my_table = wandb.Table(dataframe=df_label)\n",
    "\n",
    "wandb.log({\"Classification report\": my_table})\n",
    "\n",
    "wandb.log(\n",
    "    {\n",
    "        \"conf_mat\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=all_gts,\n",
    "            preds=all_predictions,\n",
    "            class_names=ds_train_initial.classes,\n",
    "            title=\"Confusion Matrix Test Set\",\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8baa0e-b17f-4a77-8a88-dadfdc6763ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatti che dovfrei riscontrare nei risultati:\n",
    "# capire come variano le performance al variare della depth\n",
    "# vedere che una rete più profonda senza connessioni residuali non sempre funziona meglio\n",
    "# vedere che una rete più profonda con connessioni residuali non sempre funziona meglio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243f811-8227-4c6f-b07f-56e8cd91643a",
   "metadata": {},
   "source": [
    "> ## Class Activation Mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6342267",
   "metadata": {},
   "source": [
    "Questa funzione ha prende un batch di immagini e una class label e stampa le immagini e la classi activation mapping di solo le immagini con etichetta la class label.\n",
    "Questa operazione viene ripetuta per ogni class label.\n",
    "Per calcolare la CAM occorre:\n",
    "\n",
    "- la classe target\n",
    "- i pesi dell'ultimo layer collegati alla classe target\n",
    "- la feature map in uscita dall'ultimo layer convoluzionale (in particolare il numero dei canali della feature map)\n",
    "\n",
    "![cams](./img/cam.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cam_with_images(model, x, class_target, n_img, height=32, width=32):\n",
    "    class_label = {v: k for k, v in ds_train_initial.class_to_idx.items()}\n",
    "    resize = transforms.Resize(size=(height, width), antialias=None)\n",
    "    img = resize(x)\n",
    "    # print(cam.shape) # (n°images, 1,4,4)\n",
    "    cam = model.compute_cam(x, class_target)\n",
    "    class_target = [class_target] * len(x)\n",
    "    probs = F.softmax(model(x), dim=1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, n_img, figsize=(24, 8), squeeze=False)\n",
    "\n",
    "    save_dir = f\"./cam_images/{dataset['name']}/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for b in range(n_img):\n",
    "        heatmap = cam[b].detach().cpu()\n",
    "        heatmap = resize(heatmap).permute(1, 2, 0)\n",
    "        heatmap = torch.clamp(heatmap, min=0)  # Imposta a 0 i valori negativi\n",
    "        heatmap = heatmap / heatmap.max()\n",
    "        # Normalizza tra 0 e 1 dividendo per il massimo valore\n",
    "        # Plot original image\n",
    "        img_display = (img[b].permute(1, 2, 0).cpu() + 1) / 2\n",
    "        # Normalize image to [0, 1]\n",
    "        img_display = torch.clamp(img_display, min=0, max=1)\n",
    "        ax[0, b].imshow(img_display)\n",
    "        # Plot CAM over the original image\n",
    "        img_display_heatmap = img_display * 0.5 + heatmap * 0.5\n",
    "        # Blend heatmap with image\n",
    "        ax[1, b].imshow(img_display_heatmap)\n",
    "        ax[1, b].imshow(heatmap, alpha=0.5, cmap=\"jet\")  # Plot normalized heatmap\n",
    "        ax[1, b].set_xlabel(\n",
    "            f\"Pr({class_label[class_target[b]]})={probs[b][class_target[b]].item()*100:.2f}%\",\n",
    "            color=\"black\",\n",
    "        )\n",
    "    img_name = f\"{class_target[0]}_image.png\"\n",
    "    img_path = os.path.join(save_dir, img_name)\n",
    "    plt.savefig(img_path)\n",
    "    wandb.log({f\"Sample {dataset['name']}_{class_target[0]}\": wandb.Image(img_path)})\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./cam_images/\", exist_ok=True)\n",
    "\n",
    "assert (\n",
    "    architecture[\"name\"] == \"CNN_GAP\"\n",
    "    or architecture[\"name\"] == \"PlainNetwork\"\n",
    "    or architecture[\"name\"] == \"ResNet\"\n",
    "), \"L'architettura non corrisponde a CNN_GAP, PlainNetwork o ResNet.\"\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(path_models))\n",
    "print(f'Model: {architecture[\"name\"]}\\n')\n",
    "x, y = next(iter(dl_test))  # retrieve first batch of my dataloader\n",
    "label_targets = torch.arange(0, dataset[\"num_classes\"], 1).tolist()  # labels\n",
    "for label_target in label_targets:\n",
    "    imgs_by_label = []\n",
    "    for img, label in zip(x, y):\n",
    "        if label.item() == label_target:\n",
    "            imgs_by_label.append(img)\n",
    "    print(f\"Img with target {label_target}: {len(imgs_by_label)}\")\n",
    "\n",
    "    imgs_by_label = torch.stack(imgs_by_label).to(device)\n",
    "\n",
    "    plot_cam_with_images(\n",
    "        model, imgs_by_label, class_target=label_target, n_img=len(imgs_by_label)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le architetture devono avere necessariamente un avarage pooling dopo gli strati convoluzionali\n",
    "in pytorch puoi usare avarage pooling 2d specificando le dimensioni del kernel\n",
    "altrimenti puoi usare Adaptive avarage pooling specificando che in uscita vuoi qualcosa di dimensioni 1x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
